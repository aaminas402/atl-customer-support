[
  {
    "content": "Troubleshooting Databricks connectivity | Atlan Documentation\nSkip to main content\nOn this page\nDid you know?\nThe documentation refers to both\nSQL endpoint\nand\ninteractive cluster\nas\ncompute engine\nbelow.\nDoes Atlan consider expensive queries and compute costs?\nâ\nNo, Atlan doesn't factor in expensive queries or compute costs due to limitations in the Databricks APIs, which don't expose this information.\nHow does Atlan calculate popularity for Databricks assets?\nâ\nAtlan calculates popularity for\ntables\n,\nviews\n, and\ncolumns\nin Databricks by analyzing query execution data. It retrieves query history from the\nsystem.query.history\ntable and specifically filters for\nexecution_status = 'FINISHED'\nand\nstatement_type = 'SELECT'\nto determine how frequently assets are accessed.\nHow to debug test authentication and preflight check errors?\nâ\nHostname resolution error\nProvided Host name cannot be resolved via DNS, please check and try again.\nThe hostname you have provided can't be resolved through DNS. Check that the hostname is correct.\nVerify that the DNS settings have been configured properly.\nInvalid client ID or secret\nProvided Client ID is invalid, please check and try again.",
    "metadata": {
      "source_url": "apps_connectors_data-warehouses_databricks_troubleshooting_troubleshooting-databricks-connectivity.html",
      "source_type": "docs",
      "file": "apps_connectors_data-warehouses_databricks_troubleshooting_troubleshooting-databricks-connectivity.json",
      "chunk_id": 0
    }
  },
  {
    "content": "Verify that the DNS settings have been configured properly.\nInvalid client ID or secret\nProvided Client ID is invalid, please check and try again.\nThe client ID or secret you have provided is either invalid or no longer working. Follow the steps for\nAWS\nor\nAzure\nsetup to generate new credentials.\nInvalid tenant ID\nProvided tenant ID is invalid, please check and try again.\nThe tenant ID you have provided is incorrect.\nEnsure that the tenant ID you have provided corresponds to the one in your\nMicrosoft Entra ID application\n.\nUnity Catalog not linked\nConfigured Databricks instance doesn't have Unity Catalog linked. Please choose JDBC extraction instead of REST API in Atlan.\nIf you have not set up Unity Catalog in your Databricks workspace, you can\nchange the extraction method to JDBC\ninstead of REST API to crawl your Databricks assets in Atlan.\nConnection timeout\nFailed to connect to Databricks (connection timed out). Please check your host and port and try again.\nThe connection to the Databricks instance has timed out.\nVerify that the host and port are correct.\nCheck that no firewall rules or network issues are blocking the connection.\nInvalid HTTP path",
    "metadata": {
      "source_url": "apps_connectors_data-warehouses_databricks_troubleshooting_troubleshooting-databricks-connectivity.html",
      "source_type": "docs",
      "file": "apps_connectors_data-warehouses_databricks_troubleshooting_troubleshooting-databricks-connectivity.json",
      "chunk_id": 1
    }
  },
  {
    "content": "The connection to the Databricks instance has timed out.\nVerify that the host and port are correct.\nCheck that no firewall rules or network issues are blocking the connection.\nInvalid HTTP path\nProvided HTTP path is invalid, please check and try again.\nThe\nHTTP path\nyou have provided is invalid.\nEnsure that the endpoint is properly configured and accessible, and the warehouse ID in the HTTP path is correct.\nInvalid personal access token\nPAT token is invalid, please check and try again.\nThe personal access token used for authentication is invalid.\nEnsure that the token is valid and neither deleted nor expired.\nYou can also generate a new\npersonal access token\n, if needed.\nInsufficient permisions for crawling metadata\nUser doesn't have access to any schemas / dbs, please check the accesses provided to the atlan user and try again.\nCheck that the service principal or the user who's PAT token is being used has the necessary permissions provided. Refer to the\nsetup doc\nto understand permissions required for different auth types.\nInsufficient permisions for some of the included crawling metadata",
    "metadata": {
      "source_url": "apps_connectors_data-warehouses_databricks_troubleshooting_troubleshooting-databricks-connectivity.html",
      "source_type": "docs",
      "file": "apps_connectors_data-warehouses_databricks_troubleshooting_troubleshooting-databricks-connectivity.json",
      "chunk_id": 2
    }
  },
  {
    "content": "setup doc\nto understand permissions required for different auth types.\nInsufficient permisions for some of the included crawling metadata\nWarning, user doesn't have access to the following objects anymore, or the objects no longer exist on the source!, check failed for ...\nuser doesn't have access to one or more db objects from the include filter, (such as catalogs / schemas).\nYou can either remove these objects from the include filter if they no longer exist on the source.\nOr check that the service principal or the user who's PAT token is being used has the necessary permissions provided. Refer to the\nsetup doc\nto understand permissions required for different auth types.\nInsufficient permisions to crawl tags\nUser doesn't have access to the following system tables\nCheck that you have sufficient permissions provided for the\ntags extraction\n.\nUser doesn't have permission to access warehouses\nplease check your credentials and warehouse access\nCheck that the configured user / service principal has\nCAN_USE\non the configured SQL warehouse.\nUnable to access query history from the source, user doesn't have the access\nCheck the\npermissions required",
    "metadata": {
      "source_url": "apps_connectors_data-warehouses_databricks_troubleshooting_troubleshooting-databricks-connectivity.html",
      "source_type": "docs",
      "file": "apps_connectors_data-warehouses_databricks_troubleshooting_troubleshooting-databricks-connectivity.json",
      "chunk_id": 3
    }
  },
  {
    "content": "CAN_USE\non the configured SQL warehouse.\nUnable to access query history from the source, user doesn't have the access\nCheck the\npermissions required\nfor the system tables based lineage extraction are provided.\nSystem table extraction checks failing with\nUser doesn't have access to the following system tables\nCheck the\npermissions required\nfor the system tables based extraction.\nGeneral connection failure\nUnable to connect to the configured Databricks instance, please check your credentials and configs and then try again. If the problem persists, contact\n[email protected]\n.\nCheck that you have entered the host and port correctly.\nVerify that the credentials for the connection are correct.\nCheck that your Databricks instance is properly configured and available.\nIf the problem still persists after verifying all of the previous steps,\ncontact Atlan support\n.\nWhy does the workflow take longer than usual in the extraction step?\nâ\nCertain Databricks runtime versions don't have an easy way to extract some metadata (for example partitioning, table_type, and format). Extra operations must be performed to retrieve these, resulting in slower performance.",
    "metadata": {
      "source_url": "apps_connectors_data-warehouses_databricks_troubleshooting_troubleshooting-databricks-connectivity.html",
      "source_type": "docs",
      "file": "apps_connectors_data-warehouses_databricks_troubleshooting_troubleshooting-databricks-connectivity.json",
      "chunk_id": 4
    }
  },
  {
    "content": "If you aren't already, you may want to try the\nUnity Catalog extraction method\n.\nWhy is some metadata missing?\nâ\nWhen using incremental extraction, consider running a one-time full extraction to capture any newly introduced metadata.\nCurrently, some metadata can't be extracted from Databricks:\nMetadata\nJDBC\nREST API\nSystem Tables\nViewCount\nand\nTableCount\n(on schemas)\nâ\nâ\nâ\nRowCount\n(on tables and views)\nâ\nâ\nâ\nTABLE_KIND\n(on tables and views)\nâ\nâ\nâ\nPARTITION_STRATEGY\n(on tables and views)\nâ\nâ\nâ\nCONSTRAINT_TYPE\n(on columns)\nâ\nâ\nâ\nPartition key (on columns)\nâ\nâ\nâ\nTable partitioning information\nâ\nâ\nâ\nBYTES\n,\nSIZEINBYTES\n(table size)\nâ\nâ\nâ\nThe team is exploring ways to bring this metadata into Atlan if Databricks supports extraction of the metadata.\nWhy doesn't my SQL work when querying Databricks?\nâ\nAtlan currently supports\nSparkSQL on Databricks runtime 7.x and above\n.\nCan I use Atlan when the Databricks compute engine isn't running?\nâ\nAtlan needs the Databricks compute engine to be running for two activities:\nCrawling assets (normal and scheduled run)\nQuerying assets (including data previews)",
    "metadata": {
      "source_url": "apps_connectors_data-warehouses_databricks_troubleshooting_troubleshooting-databricks-connectivity.html",
      "source_type": "docs",
      "file": "apps_connectors_data-warehouses_databricks_troubleshooting_troubleshooting-databricks-connectivity.json",
      "chunk_id": 5
    }
  },
  {
    "content": "â\nAtlan needs the Databricks compute engine to be running for two activities:\nCrawling assets (normal and scheduled run)\nQuerying assets (including data previews)\nIf you don't need to perform the activities listed, your experience shouldn't be affected.\nIn any other case, you'll get a downgraded experience on Atlan if the compute engine isn't running. Queries won't work as expected and a scheduled workflow might fail after a couple of retries.\nThe team recommends turning off the\nTerminate after x minutes of inactivity\noption in your cluster to avoid these problems. If you have this turned on, any of the listed activities triggers the cluster to come back online within about 30 seconds.\nWhy can't I see all the assets on Atlan that are available in Databricks?\nâ\nHave you\nexcluded the database or schema when crawling\n?\nDoes the\nDatabricks user you configured for crawling\nhave access to these other assets?\nWhy is the test authentication taking so long?\nâ\nPlease check the state of the compute engine. It must be in a\nrunning\nstate for all operations, including authentication.\nWhat limitations are there with the REST API (Unity Catalog) extraction method?\nâ",
    "metadata": {
      "source_url": "apps_connectors_data-warehouses_databricks_troubleshooting_troubleshooting-databricks-connectivity.html",
      "source_type": "docs",
      "file": "apps_connectors_data-warehouses_databricks_troubleshooting_troubleshooting-databricks-connectivity.json",
      "chunk_id": 6
    }
  },
  {
    "content": "running\nstate for all operations, including authentication.\nWhat limitations are there with the REST API (Unity Catalog) extraction method?\nâ\nCurrently, schema-level filtering and retrieving table partitioning information aren't supported.\nWhy has my workflow started to fail when it worked before?\nâ\nThis can happen if\nthe PAT you configured\nthe workflow with has since expired.\nYou will need to create a new PAT in Databricks, and then\nmodify the workflow configuration in Atlan\nwith this new PAT.\nIf you are unable to update the PAT, pause the workflow and\nreach out to us\n.\nHow do I migrate to Unity Catalog?\nâ\nCurrently Unity Catalog is in a public preview state.\nThe Databricks team is working on an automated migration to Unity Catalog.\nCurrently you must migrate individual tables manually.\nWhy are some notebooks missing from metadata extraction?\nâ\nNotebooks stored inside hidden directories (names starting with \".\" such as\n.hidden_dir/\n) are generally not returned by the\n/api/2.0/workspace/list\nAPI endpoint. This may cause missing notebook details in Atlan.\nWhy is metadata missing for some Databricks entities?\nâ",
    "metadata": {
      "source_url": "apps_connectors_data-warehouses_databricks_troubleshooting_troubleshooting-databricks-connectivity.html",
      "source_type": "docs",
      "file": "apps_connectors_data-warehouses_databricks_troubleshooting_troubleshooting-databricks-connectivity.json",
      "chunk_id": 7
    }
  },
  {
    "content": ".hidden_dir/\n) are generally not returned by the\n/api/2.0/workspace/list\nAPI endpoint. This may cause missing notebook details in Atlan.\nWhy is metadata missing for some Databricks entities?\nâ\nThe Databricks APIs used provide data only within a single configured workspace. If an entity used in lineage creation exists outside this workspace, its details won't be available via these APIs.\nDoes Atlan support nested columns beyond level 30?\nâ\nAtlan doesn't support nested columns beyond 30 levels for complex types such as Struct, Array, and Map. Columns exceeding this nesting depth aren't parsed. Instead, the deepest column level gets assigned the data type string, and its value contains a string representation of the remaining nested structure.\nFor example,\nLEVEL_31\nhas the data type\n<LEVEL_32:STRUCT<LEVEL_33:STRUCT<...>>>\n.\nWhat happens if the service principal loses access to one workspace?\nâ\nThe crawler is resilient to this scenario. During the discovery phase, it fails to connect to that specific workspace, logs it as inaccessible, and simply skips it. The process continues for all other available workspaces without failing the entire run.",
    "metadata": {
      "source_url": "apps_connectors_data-warehouses_databricks_troubleshooting_troubleshooting-databricks-connectivity.html",
      "source_type": "docs",
      "file": "apps_connectors_data-warehouses_databricks_troubleshooting_troubleshooting-databricks-connectivity.json",
      "chunk_id": 8
    }
  },
  {
    "content": "Why are assets from a specific catalog not appearing in Atlan?\nâ\nThis is almost always a permission issue. Verify that the service principal has been granted\nUSE CATALOG\n,\nBROWSE\n, and\nSELECT\npermissions on the catalog and its contents (schemas, tables).\nWhy is my lineage view incomplete?\nâ\nCheck the source and target tables of the missing lineage link. The service principal must have\nSELECT\npermissions on\nboth\ntables for lineage to be captured.\nFor more information on cross-workspace extraction setup, see\nSet up cross-workspace extraction\n.",
    "metadata": {
      "source_url": "apps_connectors_data-warehouses_databricks_troubleshooting_troubleshooting-databricks-connectivity.html",
      "source_type": "docs",
      "file": "apps_connectors_data-warehouses_databricks_troubleshooting_troubleshooting-databricks-connectivity.json",
      "chunk_id": 9
    }
  }
]