[
  {
    "content": "227 docs tagged with \"crawl\" | Atlan Documentation\nSkip to main content\nAdd descriptions\nYou can add descriptions to your assets in Atlan, including tables, views, and individual columns. You can even add a description in the form of a [README](/product/integrations). Doing so will enrich your data asset with the relevant contextual information.\nAdd options\n:::warning Who can do this? You must be an admin user in Atlan to create options for custom metadata properties.\nAutomate data profiling\nâAvailable via the Data Quality Studio package\nCan I connect to any source with an ODBC/JDBC driver?\nA number of Atlan's [supported connectors](/product/connections/references/connectors-and-capabilities) use a JDBC- or REST API-based approach for metadata extraction.Â If you are attempting to connect to a source with no native integration, [contact Atlan support](/support/submit-request) to share more details about your use case.\nCan I turn off sample data preview for the entire organization?",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 0
    }
  },
  {
    "content": "Can I turn off sample data preview for the entire organization?\nAtlan recommends that you turn off sample data preview at a connection level. For example, you can configure the [Snowflake crawler](/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake) to prevent users from previewing any Snowflake data.\nCrawl Aiven Kafka\nOnce you have [configured the Aiven Kafka permissions](/apps/connectors/messaging/aiven-kafka/how-tos/set-up-aiven-kafka), you can establish a connection between Atlan and Aiven Kafka.\nCrawl Amazon Athena\nTo crawl metadata from Amazon Athena, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl Amazon DynamoDB\nOnce you have [configured the Amazon DynamoDB permissions](/apps/connectors/database/amazon-dynamodb/how-tos/set-up-amazon-dynamodb), you can establish a connection between Atlan and Amazon DynamoDB.\nCrawl Amazon MSK\nTo crawl metadata from Amazon MSK, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl Amazon QuickSight",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 1
    }
  },
  {
    "content": "Crawl Amazon MSK\nTo crawl metadata from Amazon MSK, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl Amazon QuickSight\nOnce you have [configured the Amazon QuickSight permissions](/apps/connectors/business-intelligence/amazon-quicksight/how-tos/set-up-amazon-quicksight),.\nCrawl Amazon Redshift\nOnce you have configured the [Amazon Redshift access permissions](/apps/connectors/data-warehouses/amazon-redshift/how-tos/set-up-amazon-redshift), you can establish a connection between Atlan and Amazon Redshift.\nCrawl Apache Kafka\nLearn about crawl apache kafka.\nCrawl AWS Glue\nOnce you have configured the [AWS Glue access permissions](/apps/connectors/etl-tools/aws-glue/how-tos/set-up-aws-glue), you can establish a connection between Atlan and AWS Glue.\nCrawl BigID\nConfigure and run the Atlan BigID workflow to crawl metadata from BigID.\nCrawl Confluent Kafka\nLearn about crawl confluent kafka.\nCrawl Confluent Schema Registry",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 2
    }
  },
  {
    "content": "Crawl BigID\nConfigure and run the Atlan BigID workflow to crawl metadata from BigID.\nCrawl Confluent Kafka\nLearn about crawl confluent kafka.\nCrawl Confluent Schema Registry\nOnce you have [configured the Confluent Schema Registry access permissions](/apps/connectors/schema/confluent-schema-registry/how-tos/set-up-confluent-schema-registry), you can establish a connection between Atlan and Confluent Schema Registry.\nCrawl CrateDB\nConfigure and run the CrateDB crawler to extract metadata from your database\nCrawl Databricks\nTo crawl metadata from your Databricks instance, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl DataStax Enterprise\nCrawl DataStax Enterprise\nCrawl dbt\nOnce you have [configured a dbt Cloud service token](/apps/connectors/etl-tools/dbt/how-tos/set-up-dbt-cloud) or [uploaded your dbt Core project files to S3](/apps/connectors/etl-tools/dbt/how-tos/set-up-dbt-core), you can crawl dbt metadata into Atlan.\nCrawl Domo\nOnce you have [configured the Domo permissions](/apps/connectors/business-intelligence/domo/how-tos/set-up-domo), you can establish a connection between Atlan and Domo.",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 3
    }
  },
  {
    "content": "Crawl Domo\nOnce you have [configured the Domo permissions](/apps/connectors/business-intelligence/domo/how-tos/set-up-domo), you can establish a connection between Atlan and Domo.\nCrawl Fivetran\nLearn about crawl fivetran.\nCrawl GCS assets\nConfigure and run the GCS crawler to catalog your GCP GCS buckets and objects in Atlan.\nCrawl Google BigQuery\nOnce you have configured the [Google BigQuery user permissions](/apps/connectors/data-warehouses/google-bigquery/how-tos/set-up-google-bigquery), you can establish a connection between Atlan and Google BigQuery.\nCrawl Hive\nTo crawl metadata from Hive, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl IBM Cognos Analytics\nOnce you have [configured the IBM Cognos Analytics permissions](/apps/connectors/business-intelligence/ibm-cognos-analytics/how-tos/set-up-ibm-cognos-analytics), you can establish a connection between Atlan and IBM Cognos Analytics.\nCrawl Informatica CDI assets\nConfigure and run the crawler to discover and catalog your Informatica CDI assets\nCrawl Looker",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 4
    }
  },
  {
    "content": "Crawl Informatica CDI assets\nConfigure and run the crawler to discover and catalog your Informatica CDI assets\nCrawl Looker\nOnce you have configured the [Looker user permissions](/apps/connectors/business-intelligence/looker/how-tos/set-up-looker), you can establish a connection between Atlan and Looker.\nCrawl Matillion\nOnce you have [configured the Matillion user permissions](/apps/connectors/etl-tools/matillion/how-tos/set-up-matillion), you can establish a connection between Atlan and Matillion.\nCrawl Metabase\nOnce you have [configured the Metabase user permissions](/apps/connectors/business-intelligence/metabase/how-tos/set-up-metabase), you can establish a connection between Atlan and Metabase.\nCrawl Microsoft Azure Cosmos DB\nOnce you have [configured the Microsoft Azure Cosmos DB permissions](/apps/connectors/database/microsoft-azure-cosmos-db/how-tos/set-up-microsoft-azure-cosmos-db), you can establish a connection between Atlan and Microsoft Azure Cosmos DB.\nCrawl Microsoft Azure Data Factory\nOnce you have [configured the Microsoft Azure Data Factory permissions](/apps/connectors/etl-tools/microsoft-azure-data-factory/how-tos/set-up-microsoft-.",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 5
    }
  },
  {
    "content": "Crawl Microsoft Azure Data Factory\nOnce you have [configured the Microsoft Azure Data Factory permissions](/apps/connectors/etl-tools/microsoft-azure-data-factory/how-tos/set-up-microsoft-.\nCrawl Microsoft Azure Event Hubs\nOnce you have [configured the Microsoft Azure Event Hubs permissions](/apps/connectors/messaging/microsoft-azure-event-hubs/how-tos/set-up-microsoft-azure-event-hubs), you can establish a connection between Atlan and Microsoft Azure Event Hubs.\nCrawl Microsoft Azure Synapse Analytics\nOnce you have [configured the Microsoft Azure Synapse Analytics permissions](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics), you can establish a connection between Atlan and Microsoft Azure Synapse Analytics.\nCrawl Microsoft Power BI\nOnce you have configured the [Microsoft Power BI user permissions](/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/set-up-microsoft-power-bi), you can establish a connection between Atlan and Microsoft Power BI.\nCrawl Microsoft SQL Server",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 6
    }
  },
  {
    "content": "Crawl Microsoft SQL Server\nOnce you have configured the [Microsoft SQL Server user permissions](/apps/connectors/database/microsoft-sql-server/how-tos/set-up-microsoft-sql-server),.\nCrawl MicroStrategy\nOnce you have [configured the MicroStrategy permissions](/apps/connectors/business-intelligence/microstrategy/how-tos/set-up-microstrategy), you can establish a connection between Atlan and MicroStrategy.\nCrawl Mode\nOnce you have [configured the Mode user permissions](/apps/connectors/business-intelligence/mode/how-tos/set-up-mode), you can establish a connection between Atlan and Mode.\nCrawl MongoDB\nOnce you have [configured the MongoDB permissions](/apps/connectors/database/mongodb/how-tos/set-up-mongodb), you can establish a connection between Atlan and MongoDB.\nCrawl Monte Carlo\nOnce you have [configured the Monte Carlo permissions](/apps/connectors/observability/monte-carlo/how-tos/set-up-monte-carlo), you can establish a connection between Atlan and Monte Carlo.\nCrawl MySQL\nTo crawl metadata from MySQL, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl on-premises databases",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 7
    }
  },
  {
    "content": "Crawl MySQL\nTo crawl metadata from MySQL, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl on-premises databases\nOnce you have [set up the metadata-extractor tool](/apps/connectors/database/on-premises-databases/how-tos/set-up-on-premises-database-access), you can extract metadata from your on-premises databases using the following steps.\nCrawl on-premises Databricks\nOnce you have [set up the databricks-extractor tool](/apps/connectors/database/on-premises-databases/references/supported-connections-for-on-premises-databases), you can extract metadata from your on-premises Databricks instances by completing the following steps.\nCrawl on-premises IBM Cognos Analytics\nOnce you have [set up the cognos-extractor tool](/apps/connectors/business-intelligence/ibm-cognos-analytics/how-tos/set-up-on-premises-ibm-cognos-analytics-access), you can extract metadata from your on-premises IBM Cognos Analytics instances by completing the following steps.\nCrawl on-premises Kafka",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 8
    }
  },
  {
    "content": "Crawl on-premises Kafka\nOnce you have [set up the kafka-extractor tool](/apps/connectors/messaging/on-premises-event-buses/how-tos/set-up-on-premises-kafka-access), you can extract metadata from your on-premises Kafka instances by completing the following steps.\nCrawl on-premises Looker\nOnce you have [set up the looker-extractor tool](/apps/connectors/business-intelligence/looker/how-tos/set-up-on-premises-looker-access), you can extract metadata from your on-premises Looker instances using the following steps.\nCrawl on-premises Tableau\nOnce you have [set up the tableau-extractor tool](/apps/connectors/business-intelligence/tableau/how-tos/set-up-on-premises-tableau-access), you can extract metadata from your on-premises Tableau instances by completing the following steps.\nCrawl on-premises ThoughtSpot\nOnce you have [set up the thoughtspot-extractor tool](/apps/connectors/business-intelligence/thoughtspot/how-tos/set-up-on-premises-thoughtspot-access),.\nCrawl Oracle\nOnce you have configured the [Oracle user permissions](/apps/connectors/database/oracle/how-tos/set-up-oracle#create-user-in-oracle), you can establish a connection between Atlan and Oracle.\nCrawl PostgreSQL",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 9
    }
  },
  {
    "content": "Crawl PostgreSQL\nTo crawl metadata from PostgreSQL, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl PrestoSQL\nOnce you have configured the [PrestoSQL user permissions](/apps/connectors/database/prestosql/how-tos/set-up-prestosql), you can establish a connection between Atlan and PrestoSQL.\nCrawl Qlik Sense Cloud\nOnce you have [configured the Qlik Sense Cloud permissions](/apps/connectors/business-intelligence/qlik-sense-cloud/how-tos/set-up-qlik-sense-cloud), you can establish a connection between Atlan and Qlik Sense Cloud.\nCrawl Qlik Sense Enterprise on Windows\nOnce you have [configured the Qlik Sense Enterprise on Windows permissions](/apps/connectors/business-intelligence/qlik-sense-enterprise-on-windows/how-tos/how-.\nCrawl Redash\nOnce you have [configured the Redash permissions](/apps/connectors/business-intelligence/redash/how-tos/set-up-redash), you can establish a connection between Atlan and Redash.\nCrawl Redpanda Kafka",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 10
    }
  },
  {
    "content": "Once you have [configured the Redash permissions](/apps/connectors/business-intelligence/redash/how-tos/set-up-redash), you can establish a connection between Atlan and Redash.\nCrawl Redpanda Kafka\nOnce you have [configured the Redpanda Kafka permissions](/apps/connectors/messaging/redpanda-kafka/how-tos/set-up-redpanda-kafka), you can establish a connection between Atlan and Redpanda Kafka.\nCrawl S3 assets\nConfigure and run the S3 crawler to catalog your Amazon S3 buckets and objects in Atlan.\nCrawl Salesforce\nOnce you have configured the [Salesforce user permissions](/apps/connectors/crm/salesforce/how-tos/set-up-salesforce), you can establish a connection between Atlan and Salesforce.\nCrawl SAP ECC\nTo crawl metadata from your SAP ECC system, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl SAP HANA\nOnce you have [configured the SAP HANA permissions](/apps/connectors/database/sap-hana/how-tos/set-up-sap-hana), you can establish a connection between Atlan and SAP HANA.\nCrawl SAP S/4HANA",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 11
    }
  },
  {
    "content": "Once you have [configured the SAP HANA permissions](/apps/connectors/database/sap-hana/how-tos/set-up-sap-hana), you can establish a connection between Atlan and SAP HANA.\nCrawl SAP S/4HANA\nTo crawl metadata from your SAP S/4HANA system, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl Sigma\nOnce you have [configured the Sigma permissions](/apps/connectors/business-intelligence/sigma/how-tos/set-up-sigma), you can establish a connection between Atlan and Sigma.\nCrawl Sisense\nOnce you have [configured the Sisense permissions](/apps/connectors/business-intelligence/sisense/how-tos/set-up-sisense), you can establish a connection between Atlan and Sisense.\nCrawl Snowflake\nTo crawl metadata from Snowflake, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl Soda\nOnce you have [configured the Soda permissions](/apps/connectors/observability/soda/how-tos/set-up-soda), you can establish a connection between Atlan and Soda.\nCrawl Tableau",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 12
    }
  },
  {
    "content": "Crawl Soda\nOnce you have [configured the Soda permissions](/apps/connectors/observability/soda/how-tos/set-up-soda), you can establish a connection between Atlan and Soda.\nCrawl Tableau\nTo crawl metadata from Tableau, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nCrawl Teradata\nOnce you have configured the [Teradata user permissions](/apps/connectors/database/teradata/how-tos/set-up-teradata), you can establish a connection between Atlan and Teradata.\nCrawl ThoughtSpot\nOnce you have [configured the ThoughtSpot permissions](/apps/connectors/business-intelligence/thoughtspot/how-tos/set-up-thoughtspot), you can establish a connection between Atlan and ThoughtSpot.\nCrawl Trino\nTo crawl metadata from Trino, review the [order of operations](/product/connections/how-tos/order-workflows) and then complete the following steps.\nDisable data access\n:::warning Who can do this? You will need to be an admin user in Atlan to configure these options.\nDoes lineage only cover calculated fields for Tableau dashboards?",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 13
    }
  },
  {
    "content": "Disable data access\n:::warning Who can do this? You will need to be an admin user in Atlan to configure these options.\nDoes lineage only cover calculated fields for Tableau dashboards?\nAtlan displays upstream as well as downstream lineage for [Tableau dashboards](/apps/connectors/business-intelligence/tableau/references/what-does-atlan-crawl-f.\nEnrich Atlan through dbt\nBeyond the default mapped [dbt Cloud](/apps/connectors/etl-tools/dbt/references/what-does-atlan-crawl-from-dbt-cloud) or [dbt Core](/apps/connectors/etl-tools/dbt/references/what-does-atlan-crawl-from-dbt-core) properties, you can update any of Atlan's metadata attributes (except for `name`, `tenantId`, and `qualifiedName`) through your dbt model's `meta` property.\nextract lineage and usage from Databricks",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 14
    }
  },
  {
    "content": "extract lineage and usage from Databricks\nOnce you have [crawled assets from Databricks](/apps/connectors/data-warehouses/databricks/how-tos/crawl-databricks), you can retrieve lineage from [Unity Catalog](https://docs.databricks.com/data-governance/unity-catalog/index.html) and [usage and popularity metrics](/product/capabilities/usage-and-popularity/how-tos/interpret-usage-metrics) from [query history](https://docs.databricks.com/api/workspace/queryhistory/list) or system tables. This is supported for all [three authentication methods](/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks): personal access token, AWS service principal, and Azure service principal.\nManage Databricks tags\nYou must have a [Unity Catalog-enabled workspace](https://docs.databricks.com/en/data-governance/unity-catalog/get-started.html) and SQL warehouse configured to import Databricks tags in Atlan.\nManage dbt tags\nAtlan imports your [dbt tags](https://docs.getdbt.com/references/resource-configs/tags) and allows you to update your dbt assets with the imported tags.\nManage Google BigQuery tags",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 15
    }
  },
  {
    "content": "Manage dbt tags\nAtlan imports your [dbt tags](https://docs.getdbt.com/references/resource-configs/tags) and allows you to update your dbt assets with the imported tags.\nManage Google BigQuery tags\nAtlan imports your [Google BigQuery tags](https://docs.getdbt.com/references/resource-configs/tags) and allows you to update your Google BigQuery assets with the imported tags. Note that object tagging in Google BigQuery currently requires [Enterprise edition or higher](https://cloud.google.com/bigquery/docs/editions-intro#editions_features).\nManage Snowflake tags\nYou can import your Snowflake tags to Atlan through one-way tag sync. The synced Snowflake tags will be matched to corresponding tags in Atlan through case-insensitive name match and your Snowflake assets will be enriched with their synced tags from Snowflake.\nMine Amazon Redshift\nOnce you have [crawled assets from Amazon Redshift](/apps/connectors/data-warehouses/amazon-redshift/how-tos/crawl-amazon-redshift), you can mine its query history to construct lineage and retrieve [usage and popularity metrics](/product/capabilities/usage-and-popularity/how-tos/interpret-usage-metrics).\nMine Google BigQuery",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 16
    }
  },
  {
    "content": "Mine Google BigQuery\nOnce you have [crawled assets from Google BigQuery](/apps/connectors/data-warehouses/google-bigquery/how-tos/crawl-google-bigquery), you can mine its query history to construct lineage.\nMine Microsoft Azure Synapse Analytics\nLearn about mine microsoft azure synapse analytics.\nMine Microsoft Power BI\nOnce you have crawled assets from Microsoft Power BI, you can mine its activity events to generate usage metrics.\nMine queries through S3\nOnce you have crawled assets from a supported connector, you can mine query history.\nMine Snowflake\nOnce you have [crawled assets from Snowflake](/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake), you can mine its query history to construct lineage.\nMine Teradata\nOnce you have [crawled assets from Teradata](/apps/connectors/database/teradata/how-tos/crawl-teradata), you can mine its query history to construct lineage.\norder workflows",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 17
    }
  },
  {
    "content": "Mine Teradata\nOnce you have [crawled assets from Teradata](/apps/connectors/database/teradata/how-tos/crawl-teradata), you can mine its query history to construct lineage.\norder workflows\nThe [order of operations](/product/connections/how-tos/order-workflows#order-of-operations) you run in Atlan is important. Follow the specific workflow sequence outlined below when crawling [data tools](/product/connections/references/supported-sources). The right order particularly ensures that lineage is constructed without needing to rerun crawlers.\nPreflight checks for Aiven Kafka\nBefore [running the Aiven Kafka crawler](/apps/connectors/messaging/aiven-kafka/how-tos/crawl-aiven-kafka), you can run [preflight checks](/product/conne.\nPreflight checks for Amazon MSK\nBefore [running the Amazon MSK crawler](/apps/connectors/messaging/amazon-msk/how-tos/crawl-amazon-msk), you can run [preflight checks](/product/connecti.\nPreflight checks for Amazon QuickSight\nThe [ListAnalyses](https://docs.aws.amazon.com/quicksight/latest/APIReference/API_ListAnalyses.html) REST API is used to fetch the actual list of analyses for which the user has view permission.\nPreflight checks for Amazon Redshift",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 18
    }
  },
  {
    "content": "Preflight checks for Amazon Redshift\nBefore [running the Amazon Redshift crawler](/apps/connectors/data-warehouses/amazon-redshift/how-tos/crawl-amazon-redshift), you can run [preflight chec.\nPreflight checks for Apache Kafka\nBefore [running the Apache Kafka crawler](/apps/connectors/messaging/apache-kafka/how-tos/crawl-apache-kafka), run [preflight checks](/product/connection.\nPreflight checks for Confluent Schema Registry\nBefore [running the Confluent Schema Registry crawler](/apps/connectors/schema/confluent-schema-registry/how-tos/crawl-confluent-schema-registry), you ca.\nPreflight checks for Databricks\nBefore [running the Databricks crawler](/apps/connectors/data-warehouses/databricks/how-tos/crawl-databricks), you can run [preflight checks](/product/co.\nPreflight checks for DataStax Enterprise\nPreflight checks for DataStax Enterprise\nPreflight checks for dbt\nThis checks if manifest files are present in the provided bucket and prefix.\nPreflight checks for Domo\nAtlan uses the [DataSet API](https://developer.domo.com/portal/72ae9b3e80374-list-data-sets) to fetch dataset metadata from Domo.\nPreflight checks for Fivetran\nLearn about preflight checks for fivetran.",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 19
    }
  },
  {
    "content": "Preflight checks for Fivetran\nLearn about preflight checks for fivetran.\nPreflight checks for Google BigQuery\nEach request requires an OAuth 2.0 access token generated via the [service account key](https://cloud.google.com/docs/authentication#service-accounts).\nPreflight checks for Hive\nBefore [running the Hive crawler](/apps/connectors/database/hive/how-tos/crawl-hive), you can run [preflight checks](/product/connections/concepts/what-a.\nPreflight checks for Looker\nFirst, the list of projects in the _Include Projects_ and _Exclude Projects_ fields is determined. Next, the [Query Projects](https://developers.looker.com/api/explorer/3.1/methods/Project#get_all_projects) REST API is used to fetch the actual list of projects for which the user has [view capability](https://cloud.google.com/looker/docs/access-control-and-permission-management).\nPreflight checks for Metabase\nBefore [running the Metabase crawler](/apps/connectors/business-intelligence/metabase/how-tos/crawl-metabase), you can run [preflight checks](/product/co.\nPreflight checks for Microsoft Azure Data Factory",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 20
    }
  },
  {
    "content": "Preflight checks for Microsoft Azure Data Factory\nBefore [running the Microsoft Azure Data Factory crawler](/apps/connectors/etl-tools/microsoft-azure-data-factory/how-tos/crawl-microsoft-azure-data-fact.\nPreflight checks for Microsoft Azure Synapse Analytics\nThis check is performed for both [basic](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics) and [service principal](/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics) authentication method.\nPreflight checks for Microsoft Power BI\nBefore [running the Microsoft Power BI crawler](/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/crawl-microsoft-power-bi), you can run.\nPreflight checks for Microsoft SQL Server\nBefore [running the Microsoft SQL Server crawler](/apps/connectors/database/microsoft-sql-server/how-tos/crawl-microsoft-sql-server), you can run [prefli.\nPreflight checks for MicroStrategy",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 21
    }
  },
  {
    "content": "Before [running the Microsoft SQL Server crawler](/apps/connectors/database/microsoft-sql-server/how-tos/crawl-microsoft-sql-server), you can run [prefli.\nPreflight checks for MicroStrategy\nFirst, the list of projects in the _Include Projects_ and _Exclude Projects_ fields is determined. Next,Â the [Get Projects REST API](https://demo.microstrategy.com/MicroStrategyLibrary/api-docs/index.html#/Projects/getProjects_1) is used to fetch the actual list of projects for which the user has permissions.\nPreflight checks for Mode\nBefore [running the Mode crawler](/apps/connectors/business-intelligence/mode/how-tos/crawl-mode), you can run [preflight checks](/product/connections/co.\nPreflight checks for Monte Carlo\nBefore [running the Monte Carlo crawler](/apps/connectors/observability/monte-carlo/how-tos/crawl-monte-carlo), you can run [preflight checks](/product/c.\nPreflight checks for MySQL\nBefore [running the MySQL crawler](/apps/connectors/database/mysql/how-tos/crawl-mysql), you can run [preflight checks](/product/connections/concepts/wha.\nPreflight checks for Oracle",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 22
    }
  },
  {
    "content": "Before [running the MySQL crawler](/apps/connectors/database/mysql/how-tos/crawl-mysql), you can run [preflight checks](/product/connections/concepts/wha.\nPreflight checks for Oracle\nBefore [running the Oracle crawler](/apps/connectors/database/oracle/how-tos/crawl-oracle), you can run [preflight checks](/product/connections/concepts/.\nPreflight checks for PostgreSQL\nBefore [running the PostgreSQL crawler](/apps/connectors/database/postgresql/how-tos/crawl-postgresql), you can run [preflight checks](/product/connectio.\nPreflight checks for PrestoSQL\nBefore [running the PrestoSQL crawler](/apps/connectors/database/prestosql/how-tos/crawl-prestosql), you can run [preflight checks](/product/connections/.\nPreflight checks for Qlik Sense Cloud\nThis check tests for access to datasets and other Qlik objects.\nPreflight checks for Redash\nBefore [running the Redash crawler](/apps/connectors/business-intelligence/redash/how-tos/crawl-redash), you can run [preflight checks](/product/connecti.\nPreflight checks for Redpanda Kafka\nBefore [running the Redpanda Kafka crawler](/apps/connectors/messaging/redpanda-kafka/how-tos/crawl-redpanda-kafka), you can run [preflight checks](/prod.",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 23
    }
  },
  {
    "content": "Preflight checks for Redpanda Kafka\nBefore [running the Redpanda Kafka crawler](/apps/connectors/messaging/redpanda-kafka/how-tos/crawl-redpanda-kafka), you can run [preflight checks](/prod.\nPreflight checks for Salesforce\nBefore [running the Salesforce crawler](/apps/connectors/crm/salesforce/how-tos/crawl-salesforce), you can run [preflight checks](/product/connections/co.\nPreflight checks for SAP S/4HANA\nPreflight checks for SAP S/4HANA <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages#private-preview\" />\nPreflight checks for Sigma\nFirst, the list of workbooks in the _Include Workbooks_Â and _Exclude Workbooks_ fields is determined. Next, the [List Workbooks](https://help.sigmacomputing.com/hc/en-us/articles/4408555666323) REST API is used to fetch the actual list of workbooks for which the user credentials have view permission.\nPreflight checks for Sisense\nAtlan uses the [Folders API](https://sisense.dev/guides/restApi/v1/?platform=linux&spec=L2023.6#/folders) to check if it's responding with a response status code 200.\nPreflight checks for Snowflake",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 24
    }
  },
  {
    "content": "Atlan uses the [Folders API](https://sisense.dev/guides/restApi/v1/?platform=linux&spec=L2023.6#/folders) to check if it's responding with a response status code 200.\nPreflight checks for Snowflake\nBefore [running the Snowflake crawler](/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake), you can run [preflight checks](/product/conne.\nPreflight checks for Soda\nLearn about preflight checks for soda\nPreflight checks for Tableau\nThe [Server Info](https://help.tableau.com/current/api/rest_api/en-us/REST/rest_api_ref_server.htm#server_info) REST API is used to fetch the `restApiVersion` value.\nPreflight checks for Teradata\nBefore [running the Teradata crawler](/apps/connectors/database/teradata/how-tos/crawl-teradata), you can run [preflight checks](/product/connections/con.\nPreflight checks for Trino\nBefore [running the Trino crawler](/apps/connectors/database/trino/how-tos/crawl-trino), you can run [preflight checks](/product/connections/concepts/wha.\nprovide SSL certificates",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 25
    }
  },
  {
    "content": "Before [running the Trino crawler](/apps/connectors/database/trino/how-tos/crawl-trino), you can run [preflight checks](/product/connections/concepts/wha.\nprovide SSL certificates\nSSL (Secure Sockets Layer) encryption helps establish a secure connection between your data source and Atlan. Atlan currently only supports SSL certificates for [crawling Tableau](/apps/connectors/business-intelligence/tableau/how-tos/crawl-tableau).\nSet up a private network link to Amazon Athena\n:::warning Who can do this? You will need your Amazon Athena or AWS administrator involved - you may not have access yourself to complete these steps.\nSet up Amazon Redshift\n:::warning Who can do this? You will need your Amazon Redshift administrator to run these commands - you may not have access yourself.\nSet up Amazon S3\nCreate AWS IAM permissions and credentials for Atlan to access and catalog your S3 buckets and objects.\nSet up AWS Glue\nLearn about set up aws glue.\nSet up BigID\nCreate a BigID system user and API token for Atlan integration.\nSet up Confluent Schema Registry",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 26
    }
  },
  {
    "content": "Set up AWS Glue\nLearn about set up aws glue.\nSet up BigID\nCreate a BigID system user and API token for Atlan integration.\nSet up Confluent Schema Registry\n:::warning Who can do this? You will probably need your Schema Registry administrator to complete these steps - you may not have access yourself.\nSet up DataStax Enterprise\nSet up DataStax Enterprise\nSet up dbt Cloud\n:::warning Who can do this? You will probably need your dbt Cloud administrator to complete these steps - you may not have access yourself.\nSet up Domo\n:::warning Who can do this? You will need your Domo administrator to complete these steps - you may not have access yourself.\nSet up Fivetran\nLearn about set up fivetran.\nSet up Google BigQuery\nYou must be a Google BigQuery administrator to run these commands. For more information, see [Google Cloud's Granting, changing, and revoking access to resources](https://cloud.google.com/iam/docs/granting-changing-revoking-access).\nSet up Google Cloud Storage\nConfigure Google Cloud Storage for secure metadata ingestion with Atlan.\nSet up Hive\n:::warning Who can do this? You will need your Hadoop administrator to run these commands - you may not have access yourself.",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 27
    }
  },
  {
    "content": "Set up Hive\n:::warning Who can do this? You will need your Hadoop administrator to run these commands - you may not have access yourself.\nSet up IBM Cognos Analytics\n:::warning Who can do this? You must be an IBM Cognos Analytics administrator to complete these steps - you may not have access yourself.\nSet up Inventory reports\nCreate Inventory report for Amazon S3 in case of inventory based ingestion through the crawler.\nSet up Looker\n:::warning Who can do this? You will probably need your Looker administrator to run these commands - you may not have access yourself.\nSet up Microsoft Azure Cosmos DB\nIf your Microsoft Azure Cosmos DB deployment includes a mix of vCore- and RU-based accounts, you must configure both to fetch metadata. You can then use the _vCore and RU_ deployment option to [crawl your Microsoft Azure Cosmos DB assets](/apps/connectors/database/microsoft-azure-cosmos-db/how-tos/crawl-microsoft-azure-cosmos-db).\nSet up Microsoft Azure Synapse Analytics\nAtlan supports crawling the following with the Microsoft Azure Synapse Analytics package:.\nSet up Microsoft SQL Server",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 28
    }
  },
  {
    "content": "Set up Microsoft Azure Synapse Analytics\nAtlan supports crawling the following with the Microsoft Azure Synapse Analytics package:.\nSet up Microsoft SQL Server\n:::warning Who can do this? You will probably need your Microsoft SQL Server administrator to run these commands - you may not have access yourself.\nSet up MicroStrategy\nAtlan supports the basic authentication method for fetching metadata from MicroStrategy. This method uses a username and password to fetch metadata.\nSet up Mode\nIf you do not see the prompts to enter details for the user above, you are probably already signed in to Mode. Sign out of Mode first, and then accept the invite in the service account email.\nSet up MongoDB\nAtlan supports the basic authentication method for fetching metadata from MongoDB. This method uses a [username and password](#create-database-user-in-mongodb) to fetch metadata.\nSet up Monte Carlo\n:::warning Who can do this? You will probably need your Monte Carlo [account owner](https://docs.getmontecarlo.com/docs/authorizationmanaged-roles-and-groups).\nSet up on-premises database access",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 29
    }
  },
  {
    "content": ":::warning Who can do this? You will probably need your Monte Carlo [account owner](https://docs.getmontecarlo.com/docs/authorizationmanaged-roles-and-groups).\nSet up on-premises database access\nIn such cases you may want to decouple the extraction of metadata from its ingestion in Atlan. This approach gives you full control over your resources and metadata transfer to Atlan.\nSet up on-premises Databricks access\nIn some cases you will not be able to expose your Databricks instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.\nSet up on-premises IBM Cognos Analytics access\n:::warning Who can do this? You will need access to a machine that can run Docker on-premises. You will also need your IBM Cognos Analytics instance details,.\nSet up on-premises Kafka access\nIn some cases you won't be able to expose your Kafka instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.\nSet up on-premises Looker access",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 30
    }
  },
  {
    "content": "Set up on-premises Looker access\nIn some cases you won't be able to expose your Looker instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.\nSet up on-premises Tableau access\nIn some cases you may not be able to expose your Tableau instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.\nSet up on-premises ThoughtSpot access\nIn some cases you will not be able to expose your ThoughtSpot instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data.\nSet up Oracle\n:::warning Who can do this? You need your Oracle database administrator or a similar role to run these commands - you may not have access yourself.\nSet up PostgreSQL\n:::warning Who can do this? You will probably need your PostgreSQL administrator to run these commands - you may not have access yourself.\nSet up SAP HANA",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 31
    }
  },
  {
    "content": "Set up PostgreSQL\n:::warning Who can do this? You will probably need your PostgreSQL administrator to run these commands - you may not have access yourself.\nSet up SAP HANA\n:::warning Who can do this? You will probably need your SAP HANA administrator to run these commands - you may not have access yourself.\nSet up Sisense\nAtlan supports the basic authentication method for fetching metadata from Sisense. This method uses a username and password to fetch metadata.\nSet up Snowflake\n:::warning Who can do this? You need your Snowflake administrator to run these commands - you may not have access yourself. :::.\nSet up Tableau\n:::warning Who can do this? You will probably need your Tableau administrator to run these commands - you may not have access yourself.\nSet up Teradata\n:::warning Who can do this? You will probably need your Teradata administrator to run these commands - you may not have access yourself.\nSet up ThoughtSpot\n:::warning Who can do this? You will probably need your ThoughtSpot instance administrator to complete these steps - you may not have access yourself.\nSet up Trino",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 32
    }
  },
  {
    "content": "Set up ThoughtSpot\n:::warning Who can do this? You will probably need your ThoughtSpot instance administrator to complete these steps - you may not have access yourself.\nSet up Trino\n:::warning Who can do this? You will probably need your Trino administrator to run these commands - you may not have access yourself.\nTroubleshooting data models\nWhat are the known limitations of data models in Atlan?\nTroubleshooting lineage\nSo you've crawled your source, and mined the queries, but lineage is missing. Why?\nupdate column metadata in Google Sheets\nOnce you've [connected Atlan with Google Sheets](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-google-sheets), you can import the column metadata for all your data assets in Atlan and make changes to them directly in Google Sheets.\nUpdate column metadata in Microsoft Excel\nOnce you've [connected Atlan with Microsoft Excel](/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-microsoft-excel), you can import the column metadata for all your data assets in Atlan and make changes to them directly in Microsoft Excel.\nview data models",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 33
    }
  },
  {
    "content": "view data models\nOnce you have [ingested your ER model assets in Atlan](/product/capabilities/data-models/concepts/what-are-data-models), you can:.\nWhat does Atlan crawl from Aiven Kafka?\nAtlan crawls and maps the following assets and properties from Aiven Kafka.\nWhat does Atlan crawl from Amazon Athena?\nAtlan crawls and maps the following assets and properties from Amazon Athena.\nWhat does Atlan crawl from Amazon DynamoDB?\nAtlan crawls and maps the following assets and properties from Amazon DynamoDB. Atlan also currently supports lineage between Amazon DynamoDB as a source to supported data warehouses as destinations, as enriched by Fivetran.\nWhat does Atlan crawl from Amazon MSK?\nAtlan crawls and maps the following assets and properties from Amazon MSK.\nWhat does Atlan crawl from Amazon MWAA/OpenLineage?\nOnce you have [integrated Amazon MWAA/OpenLineage](/apps/connectors/lineage/amazon-mwaa-openlineage/how-tos/integrate-amazon-mwaa-openlineage), you can [.\nWhat does Atlan crawl from Amazon QuickSight?\nAtlan currently supports lineage for the Amazon QuickSight connector to the following data sources:.\nWhat does Atlan crawl from Amazon Redshift?",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 34
    }
  },
  {
    "content": "What does Atlan crawl from Amazon QuickSight?\nAtlan currently supports lineage for the Amazon QuickSight connector to the following data sources:.\nWhat does Atlan crawl from Amazon Redshift?\nAtlan crawls and maps the following assets and properties from Amazon Redshift.\nWhat does Atlan crawl from Amazon S3\nComplete reference for the S3 assets and properties that Atlan crawls and maps during S3 cataloging.\nWhat does Atlan crawl from Anomalo?\nOnce you have [integrated Anomalo](/apps/connectors/observability/anomalo/how-tos/integrate-anomalo), Atlan will receive webhook events when checks are executed in Anomalo. These checks will be cataloged in Atlan to create a relationship with existing assets using the association information from the check.\nWhat does Atlan crawl from Apache Airflow/OpenLineage?\nOnce you have [integrated Apache Airflow/OpenLineage](/apps/connectors/lineage/apache-airflow-openlineage/how-tos/integrate-apache-airflow-openlineage),.\nWhat does Atlan crawl from Apache Kafka?\nAtlan crawls and maps the following assets and properties from Apache Kafka.\nWhat does Atlan crawl from Apache Spark/OpenLineage?",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 35
    }
  },
  {
    "content": "What does Atlan crawl from Apache Kafka?\nAtlan crawls and maps the following assets and properties from Apache Kafka.\nWhat does Atlan crawl from Apache Spark/OpenLineage?\nAtlan maps the following assets and properties from Apache Spark/OpenLineage. Asset lineage support depends on the data sources that OpenLineage supports.\nWhat does Atlan crawl from Astronomer/OpenLineage?\nAtlan maps the following assets and properties from Astronomer/OpenLineage. Asset lineage support depends on the [list of operators supported by OpenLineage](https://airflow.apache.org/docs/apache-airflow-providers-openlineage/1.6.0/supported_classes.html).\nWhat does Atlan crawl from AWS Glue?\nAtlan crawls and maps the following assets and properties from AWS Glue.\nWhat does Atlan crawl from BigID?\nReference guide for BigID metadata crawled by Atlan.\nWhat does Atlan crawl from Confluent Kafka?\nAtlan crawls and maps the following assets and properties from Confluent Kafka.\nWhat does Atlan crawl from CrateDB?\nComplete list of CrateDB assets and metadata properties extracted by Atlan during crawling\nWhat does Atlan crawl from Databricks?\nAtlan crawls and maps the following assets and properties from Databricks.",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 36
    }
  },
  {
    "content": "What does Atlan crawl from Databricks?\nAtlan crawls and maps the following assets and properties from Databricks.\nWhat does Atlan crawl from DataStax Enterprise?\nWhat does Atlan crawl from DataStax Enterprise?\nWhat does Atlan crawl from Domo?\nAtlan supports lineage for the following asset types:.\nWhat does Atlan crawl from Fivetran?\nLearn about what does atlan crawl from fivetran?.\nWhat does Atlan crawl from Google BigQuery?\nAtlan doesn't run any table scans. Atlan leverages the table preview options from [Google BigQuery](https://cloud.google.com/bigquery/docs/best-practices-costs#preview-data)Â that enable you to view data for free and without affecting any quotas using the `tabledata.list` API. Hence, [table](/apps/connectors/data-warehouses/google-bigquery/references/what-does-atlan-crawl-from-google-bigquery#tables) asset previews in Atlan are already cost-optimized. However, this doesn't apply to [views](/apps/connectors/data-warehouses/google-bigquery/references/what-does-atlan-crawl-from-google-bigquery#views) and [materialized views](/apps/connectors/data-warehouses/google-bigquery/references/what-does-atlan-crawl-from-google-bigquery#materialized-views).",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 37
    }
  },
  {
    "content": "What does Atlan crawl from Google Cloud Composer/OpenLineage?\nAtlan maps the following assets and properties from Google Cloud Composer/OpenLineage. Asset lineage support depends on the [list of operators supported by OpenLineage](https://airflow.apache.org/docs/apache-airflow-providers-openlineage/1.6.0/supported_classes.html).\nWhat does Atlan crawl from Google GCS\nComplete reference for the GCS assets and properties that Atlan crawls and maps during GCS cataloging.\nWhat does Atlan crawl from Hive?\nAtlan crawls and maps the following assets and properties from Hive.\nWhat does Atlan crawl from IBM Cognos Analytics?\nAtlan crawls and maps the following assets and properties from IBM Cognos Analytics.\nWhat does Atlan crawl from Looker?\nAtlan crawls and maps the following assets and properties from Looker.\nWhat does Atlan crawl from Matillion?\nAtlan crawls and maps the following assets and properties from Matillion.\nWhat does Atlan crawl from Microsoft Azure Cosmos DB?\nOnce you have [crawled Microsoft Azure Cosmos DB](/apps/connectors/database/microsoft-azure-cosmos-db/how-tos/crawl-microsoft-azure-cosmos-db), you can [.\nWhat does Atlan crawl from Microsoft Azure Data Factory?",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 38
    }
  },
  {
    "content": "What does Atlan crawl from Microsoft Azure Data Factory?\nAtlan crawls and maps the following assets and properties from Microsoft Azure Data Factory.\nWhat does Atlan crawl from Microsoft Azure Event Hubs?\nAtlan crawls and maps the following assets and properties from Microsoft Azure Event Hubs.\nWhat does Atlan crawl from Microsoft Azure Synapse Analytics?\nAtlan crawls and maps the following assets and properties from Microsoft Azure Synapse Analytics. Atlan also currently supports view-level lineage and cross-source lineage between BI tools and SQL sources.\nWhat does Atlan crawl from Microsoft Power BI?\nAtlan crawls and maps the following assets and properties from Microsoft Power BI.\nWhat does Atlan crawl from Microsoft SQL Server?\nAtlan crawls and maps the following assets and properties from Microsoft SQL Server.\nWhat does Atlan crawl from MicroStrategy?\nAtlan crawls and maps the following assets and properties from MicroStrategy.\nWhat does Atlan crawl from MongoDB?\nAtlan crawls and maps the following assets and properties from MongoDB. Atlan currently does not support lineage for MongoDB assets.\nWhat does Atlan crawl from Monte Carlo?",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 39
    }
  },
  {
    "content": "Atlan crawls and maps the following assets and properties from MongoDB. Atlan currently does not support lineage for MongoDB assets.\nWhat does Atlan crawl from Monte Carlo?\nWhat does Atlan crawl from Monte Carlo? <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages#private-preview\" />\nWhat does Atlan crawl from MySQL?\nAtlan crawls and maps the following assets and properties from MySQL.\nWhat does Atlan crawl from Oracle?\nAtlan crawls and maps the following assets and properties from Oracle.\nWhat does Atlan crawl from PostgreSQL?\nAtlan crawls and maps the following assets and properties from PostgreSQL.\nWhat does Atlan crawl from PrestoSQL?\nAtlan crawls and maps the following assets and properties from PrestoSQL.\nWhat does Atlan crawl from Qlik Sense Cloud?\nAtlan crawls and maps the following assets and properties from Qlik Sense Cloud.\nWhat does Atlan crawl from Qlik Sense Enterprise on Windows?\nAtlan crawls and maps the following assets and properties from Qlik Sense Enterprise on Windows.\nWhat does Atlan crawl from Redash?\nAtlan crawls and maps the following assets and properties from Redash.",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 40
    }
  },
  {
    "content": "What does Atlan crawl from Redash?\nAtlan crawls and maps the following assets and properties from Redash.\nWhat does Atlan crawl from Redpanda Kafka?\nAtlan crawls and maps the following assets and properties from Redpanda Kafka.\nWhat does Atlan crawl from Salesforce?\nAtlan only performs GET requests on these five endpoints:.\nWhat does Atlan crawl from SAP ECC?\nWhat does Atlan crawl from SAP ECC? <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages#private-preview\" />\nWhat does Atlan crawl from SAP S/4HANA?\nWhat does Atlan crawl from SAP S/4HANA? <Badge variant=\"preview\" text=\"Private Preview\" link=\"/get-started/references/product-release-stages#private-preview\" />\nWhat does Atlan crawl from Sisense?\nAtlan crawls and maps the following assets and properties from Sisense.\nWhat does Atlan crawl from Snowflake?\nAtlan crawls and maps the following assets and properties from Snowflake.\nWhat does Atlan crawl from Soda?",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 41
    }
  },
  {
    "content": "What does Atlan crawl from Snowflake?\nAtlan crawls and maps the following assets and properties from Snowflake.\nWhat does Atlan crawl from Soda?\nAtlan crawls datasets and then filters out all the datasets without any checks. It then crawls the checks associated with each of the datasets with checks from Soda. These checks are cataloged in Atlan to create a relationship with existing assets using the association information from the dataset.\nWhat does Atlan crawl from Tableau?\nAtlan crawls and maps the following assets and properties from Tableau.\nWhat does Atlan crawl from Teradata?\nAtlan crawls and maps the following assets and properties from Teradata.\nWhat does Atlan crawl from ThoughtSpot?\nOnce you've [crawled ThoughtSpot](/apps/connectors/business-intelligence/thoughtspot/how-tos/crawl-thoughtspot), you can [use connector-specific filters].\nWhat does Atlan crawl from Trino?\nAtlan crawls and maps the following assets and properties from Trino.\nWhat lineage does Atlan extract from Matillion?",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 42
    }
  },
  {
    "content": "What does Atlan crawl from Trino?\nAtlan crawls and maps the following assets and properties from Trino.\nWhat lineage does Atlan extract from Matillion?\nAtlan uses Matillion's metadata API to generate lineage associated with [Matillion connectors](https://www.matillion.com/connectors). This is particularly useful for creating lineage between different tools.\nWhat lineage does Atlan extract from Microsoft Azure Data Factory?\nAtlan uses the [Microsoft Azure Data Factory REST API](https://learn.microsoft.com/en-us/rest/api/datafactory/operation-groups?view=rest-datafactory-2018-06-01).\nWhat lineage does Atlan extract from Microsoft Power BI?\nThis document helps you understand how Atlan generates lineage to upstream SQL sources for your Microsoft Power BI assets using a custom query parser, and the steps you can take while developing reports and dashboards in Microsoft Power BI to create seamless lineage generation.\nWhen does Atlan become a personal data processor or subprocessor?",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 43
    }
  },
  {
    "content": "When does Atlan become a personal data processor or subprocessor?\nAtlan personnel do not have access to any customer instance unless specifically provided by the customer. Accordingly, in the event that a customer instance contains personal data and Atlan personnel are provided access to that instance, Atlan may act as a personal data processor. In addition, depending on whether the customer is a data controller or processor, Atlan may act as a data processor or subprocessor, respectively.\nWhy does the description from Salesforce not show up in Atlan?\nAtlan supports extracting and displaying description metadata for your [Salesforce objects](/apps/connectors/crm/salesforce/references/what-does-atlan-crawl-from-salesforce).",
    "metadata": {
      "source_url": "tags_crawl.html",
      "source_type": "docs",
      "file": "tags_crawl.json",
      "chunk_id": 44
    }
  }
]