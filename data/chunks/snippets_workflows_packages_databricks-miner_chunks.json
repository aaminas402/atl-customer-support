[
  {
    "content": "Databricks miner package - Developer\nSkip to content\nDatabricks miner package\nÂ¶\nThe\nDatabricks miner package\nextract lineage and usage from databricks to Atlan for discovery.\nWill create a new connection\nThis should only be used to create the workflow the first time. Each time you run this method\nit will create a new connection and new assets within that connection â€” which could lead to duplicate\nassets if you run the workflow this way multiple times with the same settings.\nInstead, when you want to re-crawl assets, re-run the existing workflow\n(see\nRe-run existing workflow\nbelow).\n6.0.0\nTo extract lineage and usage from databricks to Atlan for discovery.\nJava\nPython\nKotlin\nRaw REST API\nComing soon\nExtract lineage and usage from databricks\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\nfrom\npyatlan.client.atlan\nimport\nAtlanClient\nfrom\npyatlan.model.packages\nimport\nDatabricksMiner\nclient\n=\nAtlanClient\n()\ncrawler\n=\n(\nDatabricksMiner\n(\n# (1)\nconnection_qualified_name\n=\n\"default/databricks/1234567890\"\n# (2)\n)\n.\nrest_api\n()\n# (3)\n.\npopularity_configuration\n(\n# (4)\nstart_date\n=\n\"1234567890\"\n,\nextraction_method\n=\nDatabricksMiner\n.\nExtractionMethod\n.\nSYSTEM_TABLE\n,\nwindow_days\n=\n30",
    "metadata": {
      "source_url": "snippets_workflows_packages_databricks-miner.html",
      "source_type": "sdk",
      "file": "snippets_workflows_packages_databricks-miner.json",
      "chunk_id": 0
    }
  },
  {
    "content": "# (2)\n)\n.\nrest_api\n()\n# (3)\n.\npopularity_configuration\n(\n# (4)\nstart_date\n=\n\"1234567890\"\n,\nextraction_method\n=\nDatabricksMiner\n.\nExtractionMethod\n.\nSYSTEM_TABLE\n,\nwindow_days\n=\n30\n,\nexcluded_users\n=\n[\n\"test-user-1\"\n,\n\"test-user-2\"\n],\nwarehouse_id\n=\n\"test-warehouse-id\"\n,\n)\n.\nto_workflow\n()\n# (5)\n)\nresponse\n=\nclient\n.\nworkflow\n.\nrun\n(\ncrawler\n)\n# (6)\nBase configuration for a new Databricks miner.\nYou must provide the exact\nqualified_name\nof the Databricks\nconnection in Atlan for which you want to mine query history.\nYou can sets up the Databricks miner to use the REST API method for fetching lineage.\nYou can also utilize any of the following methods for fetching lineage:\noffline()\nbucket_name:\nname of the S3 bucket to extract data from.\nbucket_prefix:\nprefix within the S3 bucket to narrow the extraction scope.\nsystem_table()\nwarehouse_id:\nunique identifier of the SQL warehouse to be used for system table extraction.\nOptionally, you can define\npopularity_configuration()\n:\nepoch timestamp from which queries will be fetched\nfor calculating popularity. This does not affect lineage generation.\nmethod used to fetch popularity data. Defaults to\nExtractionMethod.REST_API\n.",
    "metadata": {
      "source_url": "snippets_workflows_packages_databricks-miner.html",
      "source_type": "sdk",
      "file": "snippets_workflows_packages_databricks-miner.json",
      "chunk_id": 1
    }
  },
  {
    "content": ":\nepoch timestamp from which queries will be fetched\nfor calculating popularity. This does not affect lineage generation.\nmethod used to fetch popularity data. Defaults to\nExtractionMethod.REST_API\n.\n(Optional) number of days to consider for calculating popularity metrics.\n(Optional) list of usernames to exclude from usage metrics calculations.\n(Optional) unique identifier of the SQL warehouse to use for popularity calculations.\nRequired if\nextraction_method\nis\nExtractionMethod.SYSTEM_TABLE\n.\nNow, you can convert the package into a\nWorkflow\nobject.\nRun the workflow by invoking the\nrun()\nmethod on the workflow client, passing the created object.\nWorkflows run asynchronously\nRemember that workflows run asynchronously.\nSee the\npackages and workflows introduction\nfor details on how you can check the status\nand wait until the workflow has been completed.\nComing soon\nCreate the workflow via UI only\nWe recommend creating the workflow only via the UI. To rerun an existing workflow, see the steps below.\nRe-run existing workflow\nÂ¶\n1.1.0\nTo re-run an existing workflow for databricks assets:\nJava\nPython\nKotlin\nRaw REST API\nComing soon\nRe-run existing databricks miner workflow\n1\n2\n3\n4\n5\n6\n7\n8",
    "metadata": {
      "source_url": "snippets_workflows_packages_databricks-miner.html",
      "source_type": "sdk",
      "file": "snippets_workflows_packages_databricks-miner.json",
      "chunk_id": 2
    }
  },
  {
    "content": "Re-run existing workflow\nÂ¶\n1.1.0\nTo re-run an existing workflow for databricks assets:\nJava\nPython\nKotlin\nRaw REST API\nComing soon\nRe-run existing databricks miner workflow\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nfrom\npyatlan.client.atlan\nimport\nAtlanClient\nfrom\npyatlan.model.enums\nimport\nWorkflowPackage\nclient\n=\nAtlanClient\n()\nexisting\n=\nclient\n.\nworkflow\n.\nfind_by_type\n(\n# (1)\nprefix\n=\nWorkflowPackage\n.\nDATABRICKS_LINEAGE\n,\nmax_results\n=\n5\n)\n# Determine which Databricks workflow (n)\n# from the list of results you want to re-run.\nresponse\n=\nclient\n.\nworkflow\n.\nrerun\n(\nexisting\n[\nn\n])\n# (2)\nYou can find workflows by their type using the workflow client\nfind_by_type()\nmethod and providing the\nprefix\nfor one of the packages.\nIn this example, we do so for the\nDatabricksMiner\n. (You can also specify\nthe\nmaximum number of resulting workflows\nyou want to retrieve as results.)\nOnce you've found the workflow you want to re-run,\nyou can simply call the workflow client\nrerun()\nmethod.\nOptionally, you can use\nrerun(idempotent=True)\nto avoid re-running a workflow that is already in running or in a pending state.",
    "metadata": {
      "source_url": "snippets_workflows_packages_databricks-miner.html",
      "source_type": "sdk",
      "file": "snippets_workflows_packages_databricks-miner.json",
      "chunk_id": 3
    }
  },
  {
    "content": "you can simply call the workflow client\nrerun()\nmethod.\nOptionally, you can use\nrerun(idempotent=True)\nto avoid re-running a workflow that is already in running or in a pending state.\nThis will return details of the already running workflow if found, and by default, it is set to\nFalse\n.\nWorkflows run asynchronously\nRemember that workflows run asynchronously. See the\npackages and workflows introduction\nfor details on how you can check the status and wait until the workflow has been completed.\nComing soon\nRequires multiple steps through the raw REST API\nFind the existing workflow.\nSend through the resulting re-run request.\nPOST /api/service/workflows/indexsearch\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n{\n\"from\"\n:\n0\n,\n\"size\"\n:\n5\n,\n\"query\"\n:\n{\n\"bool\"\n:\n{\n\"filter\"\n:\n[\n{\n\"nested\"\n:\n{\n\"path\"\n:\n\"metadata\"\n,\n\"query\"\n:\n{\n\"prefix\"\n:\n{\n\"metadata.name.keyword\"\n:\n{\n\"value\"\n:\n\"atlan-databricks-lineage\"\n// (1)\n}\n}\n}\n}\n}\n]\n}\n},\n\"sort\"\n:\n[\n{\n\"metadata.creationTimestamp\"\n:\n{\n\"nested\"\n:\n{\n\"path\"\n:\n\"metadata\"\n},\n\"order\"\n:\n\"desc\"\n}\n}\n],\n\"track_total_hits\"\n:\ntrue\n}\nSearching by the\natlan-databricks-lineage",
    "metadata": {
      "source_url": "snippets_workflows_packages_databricks-miner.html",
      "source_type": "sdk",
      "file": "snippets_workflows_packages_databricks-miner.json",
      "chunk_id": 4
    }
  },
  {
    "content": "// (1)\n}\n}\n}\n}\n}\n]\n}\n},\n\"sort\"\n:\n[\n{\n\"metadata.creationTimestamp\"\n:\n{\n\"nested\"\n:\n{\n\"path\"\n:\n\"metadata\"\n},\n\"order\"\n:\n\"desc\"\n}\n}\n],\n\"track_total_hits\"\n:\ntrue\n}\nSearching by the\natlan-databricks-lineage\nprefix will ensure you only find existing Databricks assets workflows.\nName of the workflow\nThe name of the workflow will be nested within the\n_source.metadata.name\nproperty of the response object.\n(Remember since this is a search, there could be multiple results, so you may want to use the other\ndetails in each result to determine which workflow you really want.)\nPOST /api/service/workflows/submit\n100\n101\n102\n103\n104\n{\n\"namespace\"\n:\n\"default\"\n,\n\"resourceKind\"\n:\n\"WorkflowTemplate\"\n,\n\"resourceName\"\n:\n\"atlan-databricks-lineage-1684500411\"\n// (1)\n}\nSend the name of the workflow as the\nresourceName\nto rerun it.\n2025-01-13\n2025-04-02\nWas this page helpful?\nThanks for your feedback!\nThanks for your feedback! Help us improve this page by using our\nfeedback form\nto provide us with more information.\nBack to top\nCookie consent\nWe use cookies to:\nAnonymously measure page views, and\nAllow you to give us one-click feedback on any page.\nWe do\nnot\ncollect or store:",
    "metadata": {
      "source_url": "snippets_workflows_packages_databricks-miner.html",
      "source_type": "sdk",
      "file": "snippets_workflows_packages_databricks-miner.json",
      "chunk_id": 5
    }
  },
  {
    "content": "to provide us with more information.\nBack to top\nCookie consent\nWe use cookies to:\nAnonymously measure page views, and\nAllow you to give us one-click feedback on any page.\nWe do\nnot\ncollect or store:\nAny personally identifiable information.\nAny information for any (re)marketing purposes.\nWith your consent, you're helping us to make our documentation better ðŸ’™\nGoogle Analytics\nAccept\nReject\nManage settings",
    "metadata": {
      "source_url": "snippets_workflows_packages_databricks-miner.html",
      "source_type": "sdk",
      "file": "snippets_workflows_packages_databricks-miner.json",
      "chunk_id": 6
    }
  }
]