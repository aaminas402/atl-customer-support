{
  "source_url": "apps_connectors_data-warehouses_snowflake_how-tos_crawl-snowflake.html",
  "text": "Crawl Snowflake | Atlan Documentation\nSkip to main content\nOn this page\nOnce you have configured the\nSnowflake user permissions\n, you can establish a connection between Atlan and Snowflake. (If you are also using\nAWS PrivateLink\nor\nAzure Private Link\nfor Snowflake, you will need to set that up first, too.)\nTo crawl metadata from Snowflake, review the\norder of operations\nand then complete the following steps.\nSelect the source\nâ\nTo select Snowflake as your source:\nIn the top right of any screen, navigate to\nNew\nand then click\nNew Workflow\n.\nFrom the list of packages, select\nSnowflake Assets\nand click on\nSetup Workflow\n.\nProvide credentials\nâ\nChoose your extraction method:\nIn\nDirect\nextraction, Atlan connects to your database and crawls metadata directly.\nIn\nOffline\nextraction, you will need to first\nextract metadata yourself\nand\nmake it available in S3\n. This is currently only supported when using the\ninformation schema extraction method to fetch metadata with basic authentication\n.\nIn\nAgent\nextraction, Atlan's secure agent executes metadata extraction within the organization's environment.\nDirect extraction method\nâ\nTo enter your Snowflake credentials:\nFor\nAccount Identifiers (Host)\n, enter the hostname,\nAWS PrivateLink endpoint\n, or\nAzure Private Link endpoint\nfor your Snowflake instance.\nFor\nAuthentication\n, choose the method you configured when\nsetting up the Snowflake user\n:\nFor\nBasic\nauthentication, enter the\nUsername\nand\nPassword\nyou configured in either Snowflake or the identity provider.\ninfo\nðª\nDid you know?\nSnowflake recommends transitioning away from basic authentication using username and password. Change to\nkey-pair authentication\nfor enhanced security. For any existing Snowflake workflows, you can\nmodify the crawler configuration\nto update the authentication method.\nFor\nKeypair\nauthentication, enter the\nUsername\n,\nEncrypted Private Key\n, and\nPrivate Key\nPassword\nyou configured. Atlan only supports encrypted private keys with a non-empty passphrase   -  generally recommended as more secure. An empty passphrase will result in workflow failures. To generate an encrypted private key, refer to\nSnowflake documentation\n.\nFor\nOkta SSO\nauthentication,Â enter the\nUsername\n,Â\nPassword\n, and\nAuthenticator\nyou configured. The\nAuthenticator\nwill be the\nOkta URL endpoint of your Okta account\n, typically in the form of\nhttps://<okta_account_name>.okta.com\n.\nFor\nRole\n, select the Snowflake role through which the crawler should run.\nFor\nWarehouse\n, select the Snowflake warehouse in which the crawler should run.\nClick\nTest Authentication\nto confirm connectivity to Snowflake using these details.\nOnce successful, at the bottom of the screen, click\nNext\n.\nOffline extraction method\nâ\nAtlan supports the\noffline extraction method\nfor fetching metadata from Snowflake. This method uses Atlan's metadata-extractor tool to fetch metadata. You will need to first\nextract the metadata\nyourself and then\nmake it available in S3\n.\nTo enter your S3 details:\nFor\nBucket name\n, enter the name of your S3 bucket. If you are reusing Atlan's S3 bucket, you can leave this blank.\nForÂ\nBucket prefix\n, enter the S3 prefix under which all the metadata files exist. These include\ndatabases.json\n,\ncolumns-<database>.json\n, and so on.\nFor\nBucket region\n, enter the name of the S3 region.\nWhen complete, at the bottom of the screen, click\nNext\n.\nConfigure the connection\nâ\nTo complete the Snowflake connection configuration:\nProvide a\nConnection Name\nthat represents your source environment. For example, you might use values like\nproduction\n,\ndevelopment\n,\ngold\n, or\nanalytics\n.\n(Optional) To change the users able to manage this connection, change the users or groups listed under\nConnection Admins\n.\ndanger\nIf you do not specify any user or group, nobody will be able to manage the connection   -  not even admins.\n(Optional) To prevent users from querying any Snowflake data, change\nAllow SQL Query\nto\nNo\n.\n(Optional) To prevent users from previewing any Snowflake data, change\nAllow Data Preview\nto\nNo\n.\nAt the bottom of the screen, click\nNext\nto proceed.\nAgent extraction method\nâ\nAtlan supports using a Secure Agent for fetching metadata from Snowflake. To use a Secure Agent, follow these steps:\nSelect the\nAgent\ntab.\nConfigure the Snowflake data source by adding the secret keys for your secret store. For details on the required fields, refer to the\nDirect extraction\nsection.\nComplete the Secure Agent configuration by following the instructions in the\nHow to configure Secure Agent for workflow execution\nguide.\nClick\nNext\nafter completing the configuration.\nConfigure the crawler\nâ\ndanger\nWhen\nmodifying\nan existing Snowflake connection, switching to a different\nextraction method\nwill delete and recreate all assets in the existing connection. If you'd like to change the extraction method,\ncontact Atlan support\nÂ for assistance.\nBefore running the Snowflake crawler, you can further configure it.\nYou must select the\nExtraction method\nyou configured when you\nset up Snowflake\n:\nFor\nInformation Schema\nÂ\nmethod\n, keep the default selection.\nChange to\nAccount Usage\nmethod\nÂ and specify the following:\nDatabase Name\nof the copied Snowflake database\nSchema Name\nof the copied\nACCOUNT_USAGE\nschema\nIncremental extraction\nPublic preview\n- Toggle incremental extraction for faster and more efficient metadata extraction.\nYou can override the defaults for any of the remaining options:\nFor\nAsset selection\n, select a filtering option:\nTo select the assets you want to include in crawling, click\nInclude by hierarchy\nand filter for assets down to the database or schema level. (This will default to all assets, if none are specified.)\nTo have the crawler include\nDatabases\n,\nSchemas\n, or\nTables & Views\nbased on a naming convention, click\nInclude by regex\nand specify a regular expression   -  for example, specifying\nATLAN_EXAMPLE_DB.*\nfor\nDatabases\nwill include all the matching databases and their child assets.\nTo select the assets you want to exclude from crawling, click\nExclude by hierarchy\nand filter for assets down to the database or schema level. (This will default to no assets, if none are specified.)Â\nTo have the crawler ignore\nDatabases\n,\nSchemas\n, or\nTables & Views\nbased on a naming convention, click\nExclude by regex\nand specify a regular expression   -  for example, specifying\nATLAN_EXAMPLE_TABLES.*\nfor\nTables & Views\nwill exclude all the matching tables and views.\nClick\n+\nto add more filters. If you add multiple filters, assets will be crawled based on matching\nall\nthe filtering conditions you have set.\nTo exclude lineage for views in Snowflake, change\nView Definition Lineage\nto\nNo\n.\nTo\nimport tags from Snowflake to Atlan\n, change\nImport Tags\nto\nYes\n. Note the following:\nIf using the\nAccount Usage\nextraction method,\ngrant the same permissions\nas required for crawling Snowflake assets to import tags and push updated tags to Snowflake.\nIf using the\nInformation Schema\nextraction method, note that Snowflake\nstores all tag objects\nin the\nACCOUNT_USAGE\nschema. You will need to\ngrant permissions on the account usage schema instead to import tags\nfrom Snowflake.\ndanger\nObject tagging in Snowflake currently requires\nEnterprise Edition or higher\n. If your organization does not have Enterprise Edition or higher and you try to import Snowflake tags to Atlan, the Snowflake connection will fail with an error   -  unable to retrieve tags.\nFor\nControl Config\n, keep\nDefault\nfor the default configuration or click\nCustom\nto further configure the crawler:\nIf you have received a custom crawler configuration from Atlan support, for\nCustom Config\n, enter the value provided. You can also:\nEnter\n{\"ignore-all-case\": true}\nto enable crawling assets with case-sensitive identifiers.\nFor\nEnable Source Level Filtering\n, click\nTrue\nto enable schema-level filtering at source or keep\nFalse\nto disable it.\nFor\nUse JDBC Internal Methods\n, click\nTrue\nto enable JDBC internal methods for data extraction or click\nFalse\nto disable it.\nFor\nExclude tables with empty data\n, change to\nYes\nto exclude any tables and corresponding columns without any data.\nFor\nExclude views\n, change to\nYes\nto exclude all views from crawling.\nDid you know?\nIf an asset appears in both the include and exclude filters, the exclude filter takes precedence.\nRun the crawler\nâ\nTo run the Snowflake crawler, after completing the steps above:\nTo check for any\npermissions or other configuration issues\nbefore running the crawler, click\nPreflight checks\n.\nYou can either:\nTo run the crawler once immediately, at the bottom of the screen, click the\nRun\nbutton.\nTo schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the\nSchedule Run\nbutton.\nOnce the crawler has completed running, you will see the assets in Atlan's asset page! ð\nNote that the Atlan crawler will currently skip any unsupported data types to ensure a successful workflow run.\nSelect the source\nProvide credentials\nConfigure the connection\nConfigure the crawler\nRun the crawler",
  "source_type": "docs"
}