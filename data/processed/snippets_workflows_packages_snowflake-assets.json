{
  "source_url": "snippets_workflows_packages_snowflake-assets.html",
  "text": "Snowflake assets package - Developer\nSkip to content\nSnowflake assets package\n¶\nThe\nSnowflake assets package\ncrawls Snowflake assets and publishes them to Atlan for discovery.\nInformation schema\n¶\nWill create a new connection\nThis should only be used to create the workflow the first time. Each time you\nrun this method it will create a new connection and new assets within that connection\n— which could lead to duplicate assets if you run the workflow this way multiple times with the same settings.\nInstead, when you want to re-crawl assets, re-run the existing workflow\n(see\nRe-run existing workflow\nbelow).\n6.0.0\n4.0.0\nTo crawl assets from Snowflake using the built-in information schema and basic authentication:\nJava\nPython\nKotlin\nRaw REST API\nInformation schema crawling of Snowflake\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\nWorkflow\ncrawler\n=\nSnowflakeCrawler\n.\ncreator\n(\n// (1)\nclient\n,\n// (2)\n\"production\"\n,\n// (3)\nList\n.\nof\n(\nclient\n.\ngetRoleCache\n().\ngetIdForName\n(\n\"$admin\"\n)),\n// (4)\nnull\n,\nnull\n,\ntrue\n,\n// (5)\ntrue\n,\n// (6)\n10000L\n// (7)\n)\n.\nbasicAuth\n(\n// (8)\n\"atlan-user\"\n,\n// (9)\n\"atlan-pass\"\n,\n// (10)\n\"Transformer\"\n,\n// (11)\n\"COMPUTE_WH\"\n// (12)\n)\n.\ninformationSchema\n(\n\"dev.ap-south.snowflakecomputing.com\"\n)\n// (13)\n.\ninclude\n(\n// (14)\nMap\n.\nof\n(\n\"ANALYTICS\"\n,\nList\n.\nof\n(\n\"WIDE_WORLD_IMPORTERS\"\n)\n)\n)\n.\nexclude\n(\nMap\n.\nof\n())\n// (15)\n.\nlineage\n(\ntrue\n)\n// (16)\n.\ntags\n(\nfalse\n)\n// (17)\n.\nbuild\n()\n// (18)\n.\ntoWorkflow\n();\n// (19)\nWorkflowResponse\nresponse\n=\ncrawler\n.\nrun\n(\nclient\n);\n// (20)\nThe\nSnowflakeCrawler\npackage will create a workflow to crawl assets from Snowflake.\nYou must provide Atlan client.\nYou must provide a name for the connection that the Snowflake assets will exist within.\nYou must specify at\nleast one connection admin\n, either:\neveryone in a role (in this example, all\n$admin\nusers).\na list of groups (names) that will be connection admins.\na list of users (names) that will be connection admins.\nYou can specify whether you want to allow queries to this connection (\ntrue\n, as in this example) or deny all query access to the connection (\nfalse\n).\nYou can specify whether you want to allow data previews on this connection (\ntrue\n, as in this example) or deny all sample data previews to the connection (\nfalse\n).\nYou can specify a maximum number of rows that can be accessed for any asset in the connection.\nYou can also use\n.keypairAuth\nfor information schema crawling.\nYou must provide your Snowflake username.\nYou must provide your Snowflake password.\nYou must specify the name of the Snowflake role you want to use for crawling.\nYou must specify the name of the Snowflake warehouse you want to use for crawling.\nYou must provide the hostname of your Snowflake instance.\nYou can also optionally specify the set of assets to include in crawling. For Snowflake assets, this should be specified as a map keyed by database name with values as a list of schemas within that database to crawl. (If set to null, all databases and schemas will be crawled.)\nYou can also optionally specify the list of assets to exclude from crawling. For Snowflake assets, this should be specified as a map keyed by database name with values as a list of schemas within the database to exclude. (If set to null, no assets will be excluded.)\nYou can also optionally specify whether to enable lineage as part of crawling Snowflake.\nYou can also optionally specify whether to enable Snowflake tag syncing as part of crawling Snowflake.\nBuild the minimal package object.\nNow, you can convert the package into a\nWorkflow\nobject.\nYou can then run the workflow using the\nrun()\nmethod on the object you've created. Because this operation will execute work in Atlan, you must\nprovide it an\nAtlanClient\nthrough which to connect to the tenant.\nWorkflows run asynchronously\nRemember that workflows run asynchronously. See the\npackages and workflows introduction\nfor details on how you can check the status and wait until the workflow has been completed.\nInformation schema crawling of Snowflake\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\nfrom\npyatlan.client.atlan\nimport\nAtlanClient\nfrom\npyatlan.model.packages\nimport\nSnowflakeCrawler\nclient\n=\nAtlanClient\n()\ncrawler\n=\n(\nSnowflakeCrawler\n(\n# (1)\nconnection_name\n=\n\"production\"\n,\n# (2)\nadmin_roles\n=\n[\nclient\n.\nrole_cache\n.\nget_id_for_name\n(\n\"$admin\"\n)],\n# (3)\nadmin_groups\n=\nNone\n,\nadmin_users\n=\nNone\n,\nrow_limit\n=\n10000\n,\n# (4)\nallow_query\n=\nTrue\n,\n# (5)\nallow_query_preview\n=\nTrue\n,\n# (6)\n)\n.\nbasic_auth\n(\n# (7)\nusername\n=\n\"atlan-user\"\n,\n# (8)\npassword\n=\n\"atlan-pass\"\n,\n# (9)\nrole\n=\n\"Transformer\"\n,\n# (10)\nwarehouse\n=\n\"COMPUTE_WH\"\n,\n# (11)\n)\n.\ninformation_schema\n(\nhostname\n=\n\"dev.ap-south.snowflakecomputing.com\"\n)\n# (12)\n.\ninclude\n(\nassets\n=\n{\n\"ANALYTICS\"\n:\n[\n\"WIDE_WORLD_IMPORTERS\"\n]})\n# (13)\n.\nexclude\n(\nassets\n=\n{})\n# (14)\n.\nlineage\n(\nTrue\n)\n# (15)\n.\ntags\n(\nFalse\n)\n# (15)\n.\nto_workflow\n()\n# (16)\n)\nresponse\n=\nclient\n.\nworkflow\n.\nrun\n(\ncrawler\n)\n# (17)\nBase configuration for a new Snowflake crawler.\nYou must provide a name for the connection that the Snowflake assets will exist within.\nYou must specify at\nleast one connection admin\n, either:\neveryone in a role (in this example, all\n$admin\nusers).\na list of groups (names) that will be connection admins.\na list of users (names) that will be connection admins.\nYou can specify a maximum number of rows that can be accessed for any asset in the connection.\nYou can specify whether you want to allow queries to this connection.\n(\nTrue\n, as in this example) or deny all query access to the connection (\nFalse\n).\nYou can specify whether you want to allow data previews on this connection\n(\nTrue\n, as in this example) or deny all sample data previews to the connection (\nFalse\n).\nYou can also use\n.keypair_auth\nfor information schema crawling.\nYou must provide your Snowflake username.\nYou must provide your Snowflake password.\nYou must specify the name of the Snowflake role you want to use for crawling.\nYou must specify the name of the Snowflake warehouse you want to use for crawling.\nWhen using the built-in information schema, you must provide the hostname of your Snowflake instance.\nYou can also optionally specify the set of assets to include in crawling. For Snowflake assets,\nthis should be specified as a dict keyed by database name with values as a list of schemas within that\ndatabase to crawl. (If set to None, all databases and schemas will be crawled.)\nYou can also optionally specify the list of assets to exclude from crawling.\nFor Snowflake assets, this should be specified as a dict keyed by database name with values\nas a list of schemas within the database to exclude. (If set to None, no assets will be excluded.)\nYou can also optionally specify whether to enable lineage as part of crawling Snowflake.\nYou can also optionally specify whether to enable Snowflake tag syncing as part of crawling Snowflake.\nNow, you can convert the package into a\nWorkflow\nobject.\nRun the workflow by invoking the\nrun()\nmethod on the workflow client, passing the created object.\nWorkflows run asynchronously\nRemember that workflows run asynchronously. See the\npackages and workflows introduction\nfor details on how you can check the status and wait until the workflow has been completed.\nInformation schema crawling of Snowflake\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\nval\ncrawler\n=\nSnowflakeCrawler\n.\ncreator\n(\nclient\n,\n// (2)\n\"production\"\n,\n// (3)\nlistOf\n(\nclient\n.\nroleCache\n.\ngetIdForName\n(\n\"\\\n$\nadmin\n\"\n)),\n// (4)\nnull\n,\nnull\n,\ntrue\n,\n// (5)\ntrue\n,\n// (6)\n10000L\n// (7)\n)\n.\nbasicAuth\n(\n// (9)\n\"atlan-user\"\n,\n// (9)\n\"atlan-pass\"\n,\n// (10)\n\"Transformer\"\n,\n// (11)\n\"COMPUTE_WH\"\n// (12)\n)\n.\ninformationSchema\n(\n\"dev.ap-south.snowflakecomputing.com\"\n)\n// (13)\n.\ninclude\n(\n// (14)\nmapOf\n(\n\"ANALYTICS\"\nto\nlistOf\n(\n\"WIDE_WORLD_IMPORTERS\"\n)\n)\n)\n.\nexclude\n(\nmapOf\n())\n// (15)\n.\nlineage\n(\ntrue\n)\n// (16)\n.\ntags\n(\nfalse\n)\n// (17)\n.\nbuild\n()\n// (18)\n.\ntoWorkflow\n()\n// (19)\nval\nresponse\n=\ncrawler\n.\nrun\n(\nclient\n)\n// (20)\nThe\nSnowflakeCrawler\npackage will create a workflow to crawl assets from Snowflake.\nYou must provide Atlan client.\nYou must provide a name for the connection that the Snowflake assets will exist within.\nYou must specify at\nleast one connection admin\n, either:\neveryone in a role (in this example, all\n$admin\nusers).\na list of groups (names) that will be connection admins.\na list of users (names) that will be connection admins.\nYou can specify whether you want to allow queries to this connection (\ntrue\n, as in this example) or deny all query access to the connection (\nfalse\n).\nYou can specify whether you want to allow data previews on this connection (\ntrue\n, as in this example) or deny all sample data previews to the connection (\nfalse\n).\nYou can specify a maximum number of rows that can be accessed for any asset in the connection.\nYou can also use\n.keypairAuth\nfor information schema crawling.\nYou must provide your Snowflake username.\nYou must provide your Snowflake password.\nYou must specify the name of the Snowflake role you want to use for crawling.\nYou must specify the name of the Snowflake warehouse you want to use for crawling.\nYou must provide the hostname of your Snowflake instance.\nYou can also optionally specify the set of assets to include in crawling. For Snowflake assets, this should be specified as a map keyed by database name with values as a list of schemas within that database to crawl. (If set to null, all databases and schemas will be crawled.)\nYou can also optionally specify the list of assets to exclude from crawling. For Snowflake assets, this should be specified as a map keyed by database name with values as a list of schemas within the database to exclude. (If set to null, no assets will be excluded.)\nYou can also optionally specify whether to enable lineage as part of crawling Snowflake.\nYou can also optionally specify whether to enable Snowflake tag syncing as part of crawling Snowflake.\nBuild the minimal package object.\nNow, you can convert the package into a\nWorkflow\nobject.\nYou can then run the workflow using the\nrun()\nmethod on the object you've created. Because this operation will execute work in Atlan, you must\nprovide it an\nAtlanClient\nthrough which to connect to the tenant.\nWorkflows run asynchronously\nRemember that workflows run asynchronously. See the\npackages and workflows introduction\nfor details on how you can check the status and wait until the workflow has been completed.\nCreate the workflow via UI only\nWe recommend creating the workflow only via the UI. To rerun an existing workflow, see the steps below.\nAccount usage\n¶\nWill create a new connection\nThis should only be used to create the workflow the first time\nEach time you run this method it will create a new connection and new assets within that connection\n— which could lead to duplicate assets if you run the workflow this way multiple times with the same settings.\nInstead, when you want to re-crawl assets, re-run the existing workflow\n(see\nRe-run existing workflow\nbelow).\n7.0.0\n4.0.0\nTo crawl assets from Snowflake using the account usage and keypair authentication:\nJava\nPython\nKotlin\nRaw REST API\nAccount usage crawling of Snowflake\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\nWorkflow\ncrawler\n=\nSnowflakeCrawler\n.\ncreator\n(\n// (1)\nclient\n,\n// (2)\n\"production\"\n,\n// (3)\nList\n.\nof\n(\nclient\n.\ngetRoleCache\n().\ngetIdForName\n(\n\"$admin\"\n)),\n// (4)\nnull\n,\nnull\n,\ntrue\n,\n// (5)\ntrue\n,\n// (6)\n10000L\n// (7)\n)\n.\nkeypairAuth\n(\n// (8)\n\"atlan-user\"\n,\n// (9)\n\"private-key\"\n,\n// (10)\n\"private-key-pass\"\n,\n// (11)\n\"Transformer\"\n,\n// (12)\n\"COMPUTE_WH\"\n,\n// (13)\n)\n.\naccountUsage\n(\n// (14)\n\"dev.ap-south.snowflakecomputing.com\"\n,\n\"db\"\n,\n\"schema\"\n,\n)\n.\ninclude\n(\n// (15)\nMap\n.\nof\n(\n\"ANALYTICS\"\n,\nList\n.\nof\n(\n\"WIDE_WORLD_IMPORTERS\"\n)\n)\n)\n.\nexclude\n(\nMap\n.\nof\n())\n// (16)\n.\nlineage\n(\ntrue\n)\n// (17)\n.\ntags\n(\nfalse\n)\n// (18)\n.\nbuild\n()\n// (19)\n.\ntoWorkflow\n();\n// (20)\nWorkflowResponse\nresponse\n=\ncrawler\n.\nrun\n(\nclient\n);\n// (21)\nThe\nSnowflakeCrawler\npackage will create a workflow to crawl assets from Snowflake.\nYou must provide Atlan client.\nYou must provide a name for the connection that the Snowflake assets will exist within.\nYou must specify at\nleast one connection admin\n, either:\neveryone in a role (in this example, all\n$admin\nusers).\na list of groups (names) that will be connection admins.\na list of users (names) that will be connection admins.\nYou can specify whether you want to allow queries to this connection (\ntrue\n, as in this example) or deny all query access to the connection (\nfalse\n).\nYou can specify whether you want to allow data previews on this connection (\ntrue\n, as in this example) or deny all sample data previews to the connection (\nfalse\n).\nYou can specify a maximum number of rows that can be accessed for any asset in the connection.\nYou can also use\n.basicAuth\nfor account usage crawling.\nYou must provide your Snowflake username.\nYou must provide encrypted private key for authenticating with Snowflake.\nYou must provide password for the encrypted private key.\nYou must specify the name of the Snowflake role you want to use for crawling.\nYou must specify the name of the Snowflake warehouse you want to use for crawling.\nTo configure the crawler for extracting data from Snowflake's account usage database and schema, provide the following information:\nhostname of your Snowflake instance.\nname of the database to use.\nname of the schema to use.\nYou can also optionally specify the set of assets to include in crawling. For Snowflake assets, this should be specified as a map keyed by database name with values as a list of schemas within that database to crawl. (If set to null, all databases and schemas will be crawled.)\nYou can also optionally specify the set of assets to exclude from crawling. For Snowflake assets, this should be specified as a map keyed by database name with values as a list of schemas within the database to exclude. (If set to null, no assets will be excluded.)\nYou can also optionally specify whether to enable lineage as part of crawling Snowflake.\nYou can also optionally specify whether to enable Snowflake tag syncing as part of crawling Snowflake.\nBuild the minimal package object.\nNow, you can convert the package into a\nWorkflow\nobject.\nYou can then run the workflow using the\nrun()\nmethod on the object you've created. Because this operation will execute work in Atlan, you must\nprovide it an\nAtlanClient\nthrough which to connect to the tenant.\nWorkflows run asynchronously\nRemember that workflows run asynchronously. See the\npackages and workflows introduction\nfor details on how you can check the status and wait until the workflow has been completed.\nAccount usage crawling of Snowflake\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\nfrom\npyatlan.client.atlan\nimport\nAtlanClient\nfrom\npyatlan.model.packages\nimport\nSnowflakeCrawler\nclient\n=\nAtlanClient\n()\ncrawler\n=\n(\nSnowflakeCrawler\n(\n# (1)\nclient\n=\nclient\n,\n# (2)\nconnection_name\n=\n\"production\"\n,\n# (3)\nadmin_roles\n=\n[\nclient\n.\nrole_cache\n.\nget_id_for_name\n(\n\"$admin\"\n)],\n# (4)\nadmin_groups\n=\nNone\n,\nadmin_users\n=\nNone\n,\nrow_limit\n=\n10000\n,\n# (5)\nallow_query\n=\nTrue\n,\n# (6)\nallow_query_preview\n=\nTrue\n,\n# (7)\n)\n.\nkeypair_auth\n(\n# (8)\nusername\n=\n\"atlan-user\"\n,\n# (9)\nprivate_key\n=\n\"private-key\"\n,\n# (10)\nprivate_key_password\n=\n\"private-key-pass\"\n,\n# (11)\nrole\n=\n\"Transformer\"\n,\n# (12)\nwarehouse\n=\n\"COMPUTE_WH\"\n,\n# (13)\n)\n.\naccount_usage\n(\n# (14)\nhostname\n=\n\"dev.ap-south.snowflakecomputing.com\"\n,\ndatabase_name\n=\n\"db\"\n,\nschema_name\n=\n\"schema\"\n,\n)\n.\ninclude\n(\nassets\n=\n{\n\"ANALYTICS\"\n:\n[\n\"WIDE_WORLD_IMPORTERS\"\n]})\n# (15)\n.\nexclude\n(\nassets\n=\n{})\n# (16)\n.\nlineage\n(\nTrue\n)\n# (17)\n.\ntags\n(\nFalse\n)\n# (18)\n.\nto_workflow\n()\n# (19)\n)\nresponse\n=\nclient\n.\nworkflow\n.\nrun\n(\ncrawler\n)\n# (20)\nBase configuration for a new Snowflake crawler.\nYou must provide a client instance.\nYou must provide a name for the connection that the Snowflake assets will exist within.\nYou must specify at\nleast one connection admin\n, either:\neveryone in a role (in this example, all\n$admin\nusers).\na list of groups (names) that will be connection admins.\na list of users (names) that will be connection admins.\nYou can specify a maximum number of rows that can be accessed for any asset in the connection.\nYou can specify whether you want to allow queries to this connection\n(\nTrue\n, as in this example) or deny all query access to the connection (\nFalse\n).\nYou can specify whether you want to allow data previews on this connection\n(\nTrue\n, as in this example) or deny all sample data previews to the connection (\nFalse\n).\nYou can also use\n.basic_auth\nfor account usage crawling.\nYou must provide your Snowflake username.\nYou must provide encrypted private key for authenticating with Snowflake.\nYou must provide password for the encrypted private key.\nYou must specify the name of the Snowflake role you want to use for crawling.\nYou must specify the name of the Snowflake warehouse you want to use for crawling.\nTo configure the crawler for extracting data from Snowflake's\naccount usage database and schema, provide the following information:\nhostname of your Snowflake instance.\nname of the database to use.\nname of the schema to use.\nYou can also optionally specify the set of assets to include in crawling. For Snowflake assets,\nthis should be specified as a dict keyed by database name with values as a list of schemas within that\ndatabase to crawl. (If set to None, all databases and schemas will be crawled.)\nYou can also optionally specify the set of assets to exclude from crawling.\nFor Snowflake assets, this should be specified as a dict keyed by database name with values\nas a list of schemas within the database to exclude. (If set to None, no assets will be excluded.)\nYou can also optionally specify whether to enable lineage as part of crawling Snowflake.\nYou can also optionally specify whether to enable Snowflake tag syncing as part of crawling Snowflake.\nNow, you can convert the package into a\nWorkflow\nobject.\nRun the workflow by invoking the\nrun()\nmethod on the workflow client, passing the created object.\nWorkflows run asynchronously\nRemember that workflows run asynchronously. See the\npackages and workflows introduction\nfor details on how you can check the status and wait until the workflow has been completed.\nAccount usage crawling of Snowflake\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\nval\ncrawler\n=\nSnowflakeCrawler\n.\ncreator\n(\nclient\n,\n// (2)\n\"production\"\n,\n// (3)\nlistOf\n(\nclient\n.\ngetRoleCache\n().\ngetIdForName\n(\n\"\\\n$\nadmin\n\"\n)),\n// (4)\nnull\n,\nnull\n,\ntrue\n,\n// (5)\ntrue\n,\n// (6)\n10000L\n// (7)\n)\n.\nkeypairAuth\n(\n// (8)\n\"atlan-user\"\n,\n// (9)\n\"private-key\"\n,\n// (10)\n\"private-key-pass\"\n,\n// (11)\n\"Transformer\"\n,\n// (12)\n\"COMPUTE_WH\"\n,\n// (13)\n)\n.\naccountUsage\n(\n// (14)\n\"dev.ap-south.snowflakecomputing.com\"\n,\n\"db\"\n,\n\"schema\"\n,\n)\n.\ninclude\n(\n// (15)\nmapOf\n(\n\"ANALYTICS\"\nto\nlistOf\n(\n\"WIDE_WORLD_IMPORTERS\"\n)\n)\n)\n.\nexclude\n(\nemptyMap\n())\n// (16)\n.\nlineage\n(\ntrue\n)\n// (17)\n.\ntags\n(\nfalse\n)\n// (18)\n.\nbuild\n()\n// (19)\n.\ntoWorkflow\n()\n// (20)\nval\nresponse\n=\ncrawler\n.\nrun\n(\nclient\n)\n// (21)\nThe\nSnowflakeCrawler\npackage will create a workflow to crawl assets from Snowflake.\nYou must provide Atlan client.\nYou must provide a name for the connection that the Snowflake assets will exist within.\nYou must specify at\nleast one connection admin\n, either:\neveryone in a role (in this example, all\n$admin\nusers).\na list of groups (names) that will be connection admins.\na list of users (names) that will be connection admins.\nYou can specify whether you want to allow queries to this connection (\ntrue\n, as in this example) or deny all query access to the connection (\nfalse\n).\nYou can specify whether you want to allow data previews on this connection (\ntrue\n, as in this example) or deny all sample data previews to the connection (\nfalse\n).\nYou can specify a maximum number of rows that can be accessed for any asset in the connection.\nYou can also use\n.basicAuth\nfor account usage crawling.\nYou must provide your Snowflake username.\nYou must provide encrypted private key for authenticating with Snowflake.\nYou must provide password for the encrypted private key.\nYou must specify the name of the Snowflake role you want to use for crawling.\nYou must specify the name of the Snowflake warehouse you want to use for crawling.\nTo configure the crawler for extracting data from Snowflake's account usage database and schema, provide the following information:\nhostname of your Snowflake instance.\nname of the database to use.\nname of the schema to use.\nYou can also optionally specify the set of assets to include in crawling. For Snowflake assets, this should be specified as a map keyed by database name with values as a list of schemas within that database to crawl. (If set to null, all databases and schemas will be crawled.)\nYou can also optionally specify the set of assets to exclude from crawling. For Snowflake assets, this should be specified as a map keyed by database name with values as a list of schemas within the database to exclude. (If set to null, no assets will be excluded.)\nYou can also optionally specify whether to enable lineage as part of crawling Snowflake.\nYou can also optionally specify whether to enable Snowflake tag syncing as part of crawling Snowflake.\nBuild the minimal package object.\nNow, you can convert the package into a\nWorkflow\nobject.\nYou can then run the workflow using the\nrun()\nmethod on the object you've created. Because this operation will execute work in Atlan, you must\nprovide it an\nAtlanClient\nthrough which to connect to the tenant.\nWorkflows run asynchronously\nRemember that workflows run asynchronously. See the\npackages and workflows introduction\nfor details on how you can check the status and wait until the workflow has been completed.\nCreate the workflow via UI only\nWe recommend creating the workflow only via the UI. To rerun an existing workflow, see the steps below.\nRe-run existing workflow\n¶\n1.9.5\n4.0.0\nTo re-run an existing workflow for Snowflake assets:\nJava\nPython\nKotlin\nRaw REST API\nRe-run existing Snowflake workflow\n1\n2\n3\n4\nList\n<\nWorkflowSearchResult\n>\nexisting\n=\nWorkflowSearchRequest\n// (1)\n.\nfindByType\n(\nclient\n,\nSnowflakeCrawler\n.\nPREFIX\n,\n5\n);\n// (2)\n// Determine which of the results is the Snowflake workflow you want to re-run...\nWorkflowRunResponse\nresponse\n=\nexisting\n.\nget\n(\nn\n).\nrerun\n(\nclient\n);\n// (3)\nYou can search for existing workflows through the\nWorkflowSearchRequest\nclass.\nYou can find workflows by their type using the\nfindByType()\nhelper method and providing the prefix for one of the packages. In this example, we do so for the\nSnowflakeCrawler\n. (You can also specify the maximum number of resulting workflows you want to retrieve as results.)\nOnce you've found the workflow you want to re-run, you can simply call the\nrerun()\nhelper method on the workflow search result. The\nWorkflowRunResponse\nis just a subtype of\nWorkflowResponse\nso has the same helper method to monitor progress of the workflow run. Because this operation will execute work in Atlan, you must\nprovide it an\nAtlanClient\nthrough which to connect to the tenant.\nOptionally, you can use the\nrerun(client, true)\nmethod with idempotency to avoid re-running a workflow that is already in running or in a pending state. This will return details of the already running workflow if found, and by default, it is set to\nfalse\nWorkflows run asynchronously\nRemember that workflows run asynchronously. See the\npackages and workflows introduction\nfor details on how you can check the status and wait until the workflow has been completed.\nRe-run existing Snowflake workflow\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nfrom\npyatlan.client.atlan\nimport\nAtlanClient\nfrom\npyatlan.model.enums\nimport\nWorkflowPackage\nclient\n=\nAtlanClient\n()\nexisting\n=\nclient\n.\nworkflow\n.\nfind_by_type\n(\n# (1)\nprefix\n=\nWorkflowPackage\n.\nSNOWFLAKE\n,\nmax_results\n=\n5\n)\n# Determine which Snowflake workflow (n)\n# from the list of results you want to re-run.\nresponse\n=\nclient\n.\nworkflow\n.\nrerun\n(\nexisting\n[\nn\n])\n# (2)\nYou can find workflows by their type using the workflow client\nfind_by_type()\nmethod and providing the\nprefix\nfor one of the packages.\nIn this example, we do so for the\nSnowflakeCrawler\n. (You can also specify\nthe\nmaximum number of resulting workflows\nyou want to retrieve as results.)\nOnce you've found the workflow you want to re-run,\nyou can simply call the workflow client\nrerun()\nmethod.\nOptionally, you can use\nrerun(idempotent=True)\nto avoid re-running a workflow that is already in running or in a pending state.\nThis will return details of the already running workflow if found, and by default, it is set to\nFalse\n.\nWorkflows run asynchronously\nRemember that workflows run asynchronously. See the\npackages and workflows introduction\nfor details on how you can check the status and wait until the workflow has been completed.\nRe-run existing Snowflake workflow\n1\n2\n3\n4\n5\nval\nexisting\n=\nWorkflowSearchRequest\n// (1)\n.\nfindByType\n(\nclient\n,\nSnowflakeCrawler\n.\nPREFIX\n,\n5\n)\n// (2)\n// Determine which of the results is the\n// Snowflake workflow you want to re-run...\nval\nresponse\n=\nexisting\n.\nget\n(\nn\n).\nrerun\n(\nclient\n)\n// (3)\nYou can search for existing workflows through the\nWorkflowSearchRequest\nclass.\nYou can find workflows by their type using the\nfindByType()\nhelper method and providing the prefix for one of the packages. In this example, we do so for the\nSnowflakeCrawler\n. (You can also specify the maximum number of resulting workflows you want to retrieve as results.)\nOnce you've found the workflow you want to re-run, you can simply call the\nrerun()\nhelper method on the workflow search result. The\nWorkflowRunResponse\nis just a subtype of\nWorkflowResponse\nso has the same helper method to monitor progress of the workflow run. Because this operation will execute work in Atlan, you must\nprovide it an\nAtlanClient\nthrough which to connect to the tenant.\nOptionally, you can use the\nrerun(client, true)\nmethod with idempotency to avoid re-running a workflow that is already in running or in a pending state. This will return details of the already running workflow if found, and by default, it is set to\nfalse\nWorkflows run asynchronously\nRemember that workflows run asynchronously. See the\npackages and workflows introduction\nfor details on how you can check the status and wait until the workflow has been completed.\nRequires multiple steps through the raw REST API\nFind the existing workflow.\nSend through the resulting re-run request.\nPOST /api/service/workflows/indexsearch\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n{\n\"from\"\n:\n0\n,\n\"size\"\n:\n5\n,\n\"query\"\n:\n{\n\"bool\"\n:\n{\n\"filter\"\n:\n[\n{\n\"nested\"\n:\n{\n\"path\"\n:\n\"metadata\"\n,\n\"query\"\n:\n{\n\"prefix\"\n:\n{\n\"metadata.name.keyword\"\n:\n{\n\"value\"\n:\n\"atlan-snowflake\"\n// (1)\n}\n}\n}\n}\n}\n]\n}\n},\n\"sort\"\n:\n[\n{\n\"metadata.creationTimestamp\"\n:\n{\n\"nested\"\n:\n{\n\"path\"\n:\n\"metadata\"\n},\n\"order\"\n:\n\"desc\"\n}\n}\n],\n\"track_total_hits\"\n:\ntrue\n}\nSearching by the\natlan-snowflake\nprefix will ensure you only find existing Snowflake assets workflows.\nName of the workflow\nThe name of the workflow will be nested within the\n_source.metadata.name\nproperty of the response object.\n(Remember since this is a search, there could be multiple results, so you may want to use the other details\nin each result to determine which workflow you really want.)\nPOST /api/service/workflows/submit\n100\n101\n102\n103\n104\n{\n\"namespace\"\n:\n\"default\"\n,\n\"resourceKind\"\n:\n\"WorkflowTemplate\"\n,\n\"resourceName\"\n:\n\"atlan-snowflake-1684500411\"\n// (1)\n}\nSend the name of the workflow as the\nresourceName\nto rerun it.\n2022-12-28\n2025-06-11\nWas this page helpful?\nThanks for your feedback!\nThanks for your feedback! Help us improve this page by using our\nfeedback form\nto provide us with more information.\nBack to top\nCookie consent\nWe use cookies to:\nAnonymously measure page views, and\nAllow you to give us one-click feedback on any page.\nWe do\nnot\ncollect or store:\nAny personally identifiable information.\nAny information for any (re)marketing purposes.\nWith your consent, you're helping us to make our documentation better 💙\nGoogle Analytics\nAccept\nReject\nManage settings",
  "source_type": "sdk"
}